{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5633a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import text\n",
    "import utils\n",
    "import data_utils\n",
    "import json\n",
    "import commons\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd52575",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'pretrained/config.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24dd0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abcd760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 2\n",
      "Multi-band iSTFT VITS2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/ITMO/EDLM/vosk-ft-test/.venv2/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "from text.symbols import symbols\n",
    "net_g = models.SynthesizerTrn(\n",
    "    len(symbols),\n",
    "    80,\n",
    "    config['train']['segment_size'] // config['data']['hop_length'],\n",
    "    n_speakers=config['data']['n_speakers'],\n",
    "    mas_noise_scale_initial=0.01,\n",
    "    noise_scale_delta=2e-6,\n",
    "    **config['model']).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a0febc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/ITMO/EDLM/vosk-ft-test/.venv2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QAT] Starting QAT fine-tuning of DECODER only (backend=fbgemm)\n",
      "[QAT] Removing weight norm from FULL net_g...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/ITMO/EDLM/vosk-ft-test/.venv2/lib/python3.12/site-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QAT] QAT is ENABLED for net_g.dec (convs/convT/linear via QuantWrapper).\n"
     ]
    }
   ],
   "source": [
    "from train_finetune_QAT_everything_2 import prepare_model_for_qat\n",
    "net_g = prepare_model_for_qat(net_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2ca0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88413/4058327776.py:3: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  net_g = convert(net_g.eval(), inplace=False)\n",
      "/home/michael/Documents/ITMO/EDLM/vosk-ft-test/.venv2/lib/python3.12/site-packages/torch/ao/quantization/utils.py:409: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import convert\n",
    "\n",
    "net_g = convert(net_g.eval(), inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18179334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/ITMO/EDLM/vosk-ft-test/.venv2/lib/python3.12/site-packages/torch/_utils.py:444: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: []\n",
      "Unexpected keys: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SynthesizerTrn(\n",
       "  (enc_p): TextEncoder(\n",
       "    (emb): Embedding(62, 192)\n",
       "    (encoder): Encoder(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (attn_layers): ModuleList(\n",
       "        (0): MultiHeadAttention(\n",
       "          (conv_q): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1033]), zero_point=tensor([70]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.1287403106689453, zero_point=47)\n",
       "          )\n",
       "          (conv_k): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1033]), zero_point=tensor([70]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.04974838346242905, zero_point=64)\n",
       "          )\n",
       "          (conv_v): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1033]), zero_point=tensor([70]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.056701984256505966, zero_point=60)\n",
       "          )\n",
       "          (conv_o): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0492]), zero_point=tensor([59]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.029039720073342323, zero_point=66)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): MultiHeadAttention(\n",
       "          (conv_q): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0701]), zero_point=tensor([74]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.09830820560455322, zero_point=69)\n",
       "          )\n",
       "          (conv_k): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0701]), zero_point=tensor([74]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.04944169148802757, zero_point=64)\n",
       "          )\n",
       "          (conv_v): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0701]), zero_point=tensor([74]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.029245497658848763, zero_point=58)\n",
       "          )\n",
       "          (conv_o): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0229]), zero_point=tensor([62]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.0202159583568573, zero_point=67)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): MultiHeadAttention(\n",
       "          (conv_q): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1010]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.06779397279024124, zero_point=62)\n",
       "          )\n",
       "          (conv_k): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1010]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.0884489044547081, zero_point=60)\n",
       "          )\n",
       "          (conv_v): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1010]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.05489424988627434, zero_point=69)\n",
       "          )\n",
       "          (conv_o): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0236]), zero_point=tensor([57]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.01420509722083807, zero_point=61)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): MultiHeadAttention(\n",
       "          (conv_q): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0582]), zero_point=tensor([65]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.06609193235635757, zero_point=64)\n",
       "          )\n",
       "          (conv_k): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0582]), zero_point=tensor([65]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.09076143801212311, zero_point=67)\n",
       "          )\n",
       "          (conv_v): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0582]), zero_point=tensor([65]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.040503550320863724, zero_point=55)\n",
       "          )\n",
       "          (conv_o): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0186]), zero_point=tensor([60]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.03384261950850487, zero_point=91)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): MultiHeadAttention(\n",
       "          (conv_q): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0659]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.07250835746526718, zero_point=60)\n",
       "          )\n",
       "          (conv_k): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0659]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.05528080090880394, zero_point=63)\n",
       "          )\n",
       "          (conv_v): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0659]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.03211360424757004, zero_point=69)\n",
       "          )\n",
       "          (conv_o): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0197]), zero_point=tensor([92]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.013483470305800438, zero_point=94)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): MultiHeadAttention(\n",
       "          (conv_q): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0506]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.0600762777030468, zero_point=59)\n",
       "          )\n",
       "          (conv_k): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0506]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.08328062295913696, zero_point=72)\n",
       "          )\n",
       "          (conv_v): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0506]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.04764987528324127, zero_point=66)\n",
       "          )\n",
       "          (conv_o): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0158]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.0149700827896595, zero_point=64)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layers_1): ModuleList(\n",
       "        (0-5): 6 x LayerNorm()\n",
       "      )\n",
       "      (ffn_layers): ModuleList(\n",
       "        (0): FFN(\n",
       "          (conv_1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0423]), zero_point=tensor([67]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 768, kernel_size=(3,), stride=(1,), scale=0.039994366466999054, zero_point=84)\n",
       "          )\n",
       "          (conv_2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0147]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(768, 192, kernel_size=(3,), stride=(1,), scale=0.06570848077535629, zero_point=78)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): FFN(\n",
       "          (conv_1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0383]), zero_point=tensor([44]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 768, kernel_size=(3,), stride=(1,), scale=0.02532746084034443, zero_point=85)\n",
       "          )\n",
       "          (conv_2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0089]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(768, 192, kernel_size=(3,), stride=(1,), scale=0.056330688297748566, zero_point=75)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): FFN(\n",
       "          (conv_1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0442]), zero_point=tensor([103]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 768, kernel_size=(3,), stride=(1,), scale=0.040417157113552094, zero_point=86)\n",
       "          )\n",
       "          (conv_2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0142]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(768, 192, kernel_size=(3,), stride=(1,), scale=0.2214537262916565, zero_point=77)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): FFN(\n",
       "          (conv_1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0289]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 768, kernel_size=(3,), stride=(1,), scale=0.052230533212423325, zero_point=91)\n",
       "          )\n",
       "          (conv_2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0155]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(768, 192, kernel_size=(3,), stride=(1,), scale=0.10647384822368622, zero_point=88)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): FFN(\n",
       "          (conv_1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0373]), zero_point=tensor([70]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 768, kernel_size=(3,), stride=(1,), scale=0.06314553320407867, zero_point=108)\n",
       "          )\n",
       "          (conv_2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0101]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(768, 192, kernel_size=(3,), stride=(1,), scale=0.022889966145157814, zero_point=80)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): FFN(\n",
       "          (conv_1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0646]), zero_point=tensor([97]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(192, 768, kernel_size=(3,), stride=(1,), scale=0.04007316380739212, zero_point=105)\n",
       "          )\n",
       "          (conv_2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0074]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(768, 192, kernel_size=(3,), stride=(1,), scale=0.01297734770923853, zero_point=57)\n",
       "          )\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layers_2): ModuleList(\n",
       "        (0-5): 6 x LayerNorm()\n",
       "      )\n",
       "      (spk_emb_linear): QuantWrapper(\n",
       "        (quant): Quantize(scale=tensor([0.0159]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "        (dequant): DeQuantize()\n",
       "        (module): QuantizedLinear(in_features=256, out_features=192, scale=0.026276549324393272, zero_point=68, qscheme=torch.per_tensor_affine)\n",
       "      )\n",
       "    )\n",
       "    (proj): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.0153]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.059710241854190826, zero_point=58)\n",
       "    )\n",
       "  )\n",
       "  (dec): Multiband_iSTFT_Generator(\n",
       "    (conv_pre): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.1246]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(192, 512, kernel_size=(7,), stride=(1,), scale=0.1584651619195938, zero_point=56, padding=(3,))\n",
       "    )\n",
       "    (ups): ModuleList(\n",
       "      (0): QuantWrapper(\n",
       "        (quant): Quantize(scale=tensor([0.0946]), zero_point=tensor([9]), dtype=torch.quint8)\n",
       "        (dequant): DeQuantize()\n",
       "        (module): QuantizedConvTranspose1d(512, 256, kernel_size=(16,), stride=(4,), scale=0.08025652915239334, zero_point=75, padding=(6,))\n",
       "      )\n",
       "      (1): QuantWrapper(\n",
       "        (quant): Quantize(scale=tensor([0.1011]), zero_point=tensor([15]), dtype=torch.quint8)\n",
       "        (dequant): DeQuantize()\n",
       "        (module): QuantizedConvTranspose1d(256, 128, kernel_size=(16,), stride=(4,), scale=0.05459705740213394, zero_point=61, padding=(6,))\n",
       "      )\n",
       "    )\n",
       "    (resblocks): ModuleList(\n",
       "      (0): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0367]), zero_point=tensor([16]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.08068066835403442, zero_point=64, padding=(1,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0521]), zero_point=tensor([15]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.1553778499364853, zero_point=73, padding=(3,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1508]), zero_point=tensor([21]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.5304640531539917, zero_point=94, padding=(5,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0432]), zero_point=tensor([12]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.07568184286355972, zero_point=61, padding=(1,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0713]), zero_point=tensor([15]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.3912894129753113, zero_point=86, padding=(1,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1707]), zero_point=tensor([28]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.6211645007133484, zero_point=83, padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (add_ff): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0367]), zero_point=tensor([16]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.10136760771274567, zero_point=73, padding=(3,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0722]), zero_point=tensor([10]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.11633327603340149, zero_point=84, padding=(9,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1031]), zero_point=tensor([9]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.14142045378684998, zero_point=54, padding=(15,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0476]), zero_point=tensor([15]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.10654043406248093, zero_point=66, padding=(3,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0460]), zero_point=tensor([21]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.13636241853237152, zero_point=58, padding=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0839]), zero_point=tensor([9]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.2171442210674286, zero_point=69, padding=(3,))\n",
       "          )\n",
       "        )\n",
       "        (add_ff): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0367]), zero_point=tensor([16]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.10635773837566376, zero_point=67, padding=(5,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0890]), zero_point=tensor([15]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.2024877369403839, zero_point=83, padding=(15,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0993]), zero_point=tensor([15]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.168385311961174, zero_point=77, padding=(25,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0551]), zero_point=tensor([13]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.18740388751029968, zero_point=76, padding=(5,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0797]), zero_point=tensor([20]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.20696412026882172, zero_point=83, padding=(5,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0733]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.30173009634017944, zero_point=82, padding=(5,))\n",
       "          )\n",
       "        )\n",
       "        (add_ff): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0292]), zero_point=tensor([11]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.10001739114522934, zero_point=95, padding=(1,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0452]), zero_point=tensor([11]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.11524920165538788, zero_point=70, padding=(3,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0993]), zero_point=tensor([16]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.2632269561290741, zero_point=64, padding=(5,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0315]), zero_point=tensor([29]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.06955143809318542, zero_point=73, padding=(1,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0563]), zero_point=tensor([14]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.23527568578720093, zero_point=81, padding=(1,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1396]), zero_point=tensor([12]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.2796715497970581, zero_point=72, padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (add_ff): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0292]), zero_point=tensor([11]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.1556861698627472, zero_point=70, padding=(3,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1315]), zero_point=tensor([8]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.47555235028266907, zero_point=77, padding=(9,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.2934]), zero_point=tensor([8]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.6309149861335754, zero_point=76, padding=(15,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0710]), zero_point=tensor([14]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.2068040519952774, zero_point=49, padding=(3,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.1969]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.4562070071697235, zero_point=61, padding=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.2684]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=1.0093721151351929, zero_point=103, padding=(3,))\n",
       "          )\n",
       "        )\n",
       "        (add_ff): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0292]), zero_point=tensor([11]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.07981817424297333, zero_point=81, padding=(5,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0288]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.07057281583547592, zero_point=76, padding=(15,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0675]), zero_point=tensor([9]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.17229010164737701, zero_point=85, padding=(25,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0326]), zero_point=tensor([19]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.052584804594516754, zero_point=66, padding=(5,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0314]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.102879598736763, zero_point=58, padding=(5,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0665]), zero_point=tensor([21]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.24762383103370667, zero_point=81, padding=(5,))\n",
       "          )\n",
       "        )\n",
       "        (add_ff): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (reflection_pad): ReflectionPad1d((1, 0))\n",
       "    (subband_conv_post): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.0883]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(128, 72, kernel_size=(7,), stride=(1,), scale=0.03152306005358696, zero_point=39, padding=(3,), bias=False)\n",
       "    )\n",
       "    (stft): TorchSTFT()\n",
       "  )\n",
       "  (enc_q): PosteriorEncoder(\n",
       "    (pre): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.0892]), zero_point=tensor([105]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(80, 192, kernel_size=(1,), stride=(1,), scale=0.3824197053909302, zero_point=94)\n",
       "    )\n",
       "    (enc): WN(\n",
       "      (in_layers): ModuleList(\n",
       "        (0): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3797]), zero_point=tensor([94]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.7192956209182739, zero_point=47, padding=(2,))\n",
       "        )\n",
       "        (1): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3749]), zero_point=tensor([95]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.6543861627578735, zero_point=50, padding=(2,))\n",
       "        )\n",
       "        (2): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3699]), zero_point=tensor([96]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.870606005191803, zero_point=48, padding=(2,))\n",
       "        )\n",
       "        (3): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3742]), zero_point=tensor([95]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.4138733148574829, zero_point=49, padding=(2,))\n",
       "        )\n",
       "        (4): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3773]), zero_point=tensor([95]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.487949401140213, zero_point=49, padding=(2,))\n",
       "        )\n",
       "        (5): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3819]), zero_point=tensor([93]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.383591890335083, zero_point=46, padding=(2,))\n",
       "        )\n",
       "        (6): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3872]), zero_point=tensor([92]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.3273865580558777, zero_point=71, padding=(2,))\n",
       "        )\n",
       "        (7): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3903]), zero_point=tensor([91]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.3596794009208679, zero_point=55, padding=(2,))\n",
       "        )\n",
       "        (8): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3946]), zero_point=tensor([90]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.27604004740715027, zero_point=55, padding=(2,))\n",
       "        )\n",
       "        (9): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.4003]), zero_point=tensor([87]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.39536982774734497, zero_point=59, padding=(2,))\n",
       "        )\n",
       "        (10): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.4103]), zero_point=tensor([84]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.2996094524860382, zero_point=63, padding=(2,))\n",
       "        )\n",
       "        (11): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.4116]), zero_point=tensor([83]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.37398263812065125, zero_point=82, padding=(2,))\n",
       "        )\n",
       "        (12): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.4089]), zero_point=tensor([81]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.23928377032279968, zero_point=62, padding=(2,))\n",
       "        )\n",
       "        (13): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.4016]), zero_point=tensor([81]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.34324324131011963, zero_point=59, padding=(2,))\n",
       "        )\n",
       "        (14): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3936]), zero_point=tensor([81]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.2821230888366699, zero_point=60, padding=(2,))\n",
       "        )\n",
       "        (15): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.3886]), zero_point=tensor([80]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.1741364300251007, zero_point=67, padding=(2,))\n",
       "        )\n",
       "      )\n",
       "      (res_skip_layers): ModuleList(\n",
       "        (0): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.06878194957971573, zero_point=57)\n",
       "        )\n",
       "        (1): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.1122683435678482, zero_point=71)\n",
       "        )\n",
       "        (2): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.04992562532424927, zero_point=81)\n",
       "        )\n",
       "        (3): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.018820473924279213, zero_point=70)\n",
       "        )\n",
       "        (4): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.022428035736083984, zero_point=64)\n",
       "        )\n",
       "        (5): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.022571830078959465, zero_point=60)\n",
       "        )\n",
       "        (6): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.016095716506242752, zero_point=61)\n",
       "        )\n",
       "        (7): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.02084607444703579, zero_point=72)\n",
       "        )\n",
       "        (8): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.021345578134059906, zero_point=64)\n",
       "        )\n",
       "        (9): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.03117617964744568, zero_point=63)\n",
       "        )\n",
       "        (10): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.03305777534842491, zero_point=69)\n",
       "        )\n",
       "        (11): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.03205138072371483, zero_point=55)\n",
       "        )\n",
       "        (12): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.041056130081415176, zero_point=64)\n",
       "        )\n",
       "        (13): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.051855720579624176, zero_point=62)\n",
       "        )\n",
       "        (14): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.05767703801393509, zero_point=56)\n",
       "        )\n",
       "        (15): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.011126495897769928, zero_point=66)\n",
       "        )\n",
       "      )\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "      (cond_layer): QuantWrapper(\n",
       "        (quant): Quantize(scale=tensor([0.0159]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "        (dequant): DeQuantize()\n",
       "        (module): QuantizedConv1d(256, 6144, kernel_size=(1,), stride=(1,), scale=0.04195849969983101, zero_point=55)\n",
       "      )\n",
       "    )\n",
       "    (proj): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.0879]), zero_point=tensor([69]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.11001639813184738, zero_point=63)\n",
       "    )\n",
       "  )\n",
       "  (flow): ResidualCouplingTransformersBlock(\n",
       "    (flows): ModuleList(\n",
       "      (0): ResidualCouplingTransformersLayer2(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1217]), zero_point=tensor([70]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(96, 192, kernel_size=(1,), stride=(1,), scale=0.15007291734218597, zero_point=71)\n",
       "        )\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.1468]), zero_point=tensor([71]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.10736274719238281, zero_point=53)\n",
       "              )\n",
       "              (conv_k): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.1468]), zero_point=tensor([71]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.11881164461374283, zero_point=44)\n",
       "              )\n",
       "              (conv_v): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.1468]), zero_point=tensor([71]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.1572868376970291, zero_point=58)\n",
       "              )\n",
       "              (conv_o): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0823]), zero_point=tensor([56]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.13819032907485962, zero_point=54)\n",
       "              )\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0228]), zero_point=tensor([47]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(5,), stride=(1,), scale=0.08235221356153488, zero_point=84)\n",
       "              )\n",
       "              (conv_2): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0273]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(5,), stride=(1,), scale=0.06617290526628494, zero_point=70)\n",
       "              )\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1461]), zero_point=tensor([71]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.2404351681470871, zero_point=62, padding=(2,))\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1314]), zero_point=tensor([70]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.2441512644290924, zero_point=64, padding=(2,))\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1253]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.1696772426366806, zero_point=69, padding=(2,))\n",
       "            )\n",
       "            (3): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1251]), zero_point=tensor([66]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.11890241503715515, zero_point=73, padding=(2,))\n",
       "            )\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.03991621732711792, zero_point=56)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.04127933830022812, zero_point=65)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.04045895114541054, zero_point=65)\n",
       "            )\n",
       "            (3): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0155]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.022541627287864685, zero_point=58)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0159]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 1536, kernel_size=(1,), stride=(1,), scale=0.040040746331214905, zero_point=65)\n",
       "          )\n",
       "        )\n",
       "        (post): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0440]), zero_point=tensor([55]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 96, kernel_size=(1,), stride=(1,), scale=0.10386333614587784, zero_point=62)\n",
       "        )\n",
       "      )\n",
       "      (1): Flip()\n",
       "      (2): ResidualCouplingTransformersLayer2(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1100]), zero_point=tensor([58]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(96, 192, kernel_size=(1,), stride=(1,), scale=0.11914177238941193, zero_point=73)\n",
       "        )\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.1164]), zero_point=tensor([73]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.09000209718942642, zero_point=63)\n",
       "              )\n",
       "              (conv_k): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.1164]), zero_point=tensor([73]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.1047590970993042, zero_point=64)\n",
       "              )\n",
       "              (conv_v): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.1164]), zero_point=tensor([73]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.1266310065984726, zero_point=66)\n",
       "              )\n",
       "              (conv_o): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0816]), zero_point=tensor([66]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.15956731140613556, zero_point=87)\n",
       "              )\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0210]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(5,), stride=(1,), scale=0.065327487885952, zero_point=66)\n",
       "              )\n",
       "              (conv_2): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0312]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(5,), stride=(1,), scale=0.06597939878702164, zero_point=65)\n",
       "              )\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1214]), zero_point=tensor([73]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.20320743322372437, zero_point=54, padding=(2,))\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1163]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.18031905591487885, zero_point=68, padding=(2,))\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1202]), zero_point=tensor([74]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.1275353878736496, zero_point=69, padding=(2,))\n",
       "            )\n",
       "            (3): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1263]), zero_point=tensor([75]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.1075160875916481, zero_point=62, padding=(2,))\n",
       "            )\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0156]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.03436972573399544, zero_point=58)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0156]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.03654162585735321, zero_point=65)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0154]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.038028642535209656, zero_point=66)\n",
       "            )\n",
       "            (3): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0155]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.01606329157948494, zero_point=72)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0159]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 1536, kernel_size=(1,), stride=(1,), scale=0.0402536541223526, zero_point=67)\n",
       "          )\n",
       "        )\n",
       "        (post): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0317]), zero_point=tensor([66]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 96, kernel_size=(1,), stride=(1,), scale=0.09303618222475052, zero_point=64)\n",
       "        )\n",
       "      )\n",
       "      (3): Flip()\n",
       "      (4): ResidualCouplingTransformersLayer2(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1155]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(96, 192, kernel_size=(1,), stride=(1,), scale=0.07861406356096268, zero_point=68)\n",
       "        )\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0772]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.09698166698217392, zero_point=61)\n",
       "              )\n",
       "              (conv_k): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0772]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.10579150915145874, zero_point=62)\n",
       "              )\n",
       "              (conv_v): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0772]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.13132323324680328, zero_point=62)\n",
       "              )\n",
       "              (conv_o): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0638]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.1890072226524353, zero_point=81)\n",
       "              )\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0235]), zero_point=tensor([91]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(5,), stride=(1,), scale=0.062447380274534225, zero_point=83)\n",
       "              )\n",
       "              (conv_2): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0213]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(5,), stride=(1,), scale=0.0372670479118824, zero_point=61)\n",
       "              )\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0905]), zero_point=tensor([76]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.18409647047519684, zero_point=69, padding=(2,))\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0925]), zero_point=tensor([79]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.18349231779575348, zero_point=73, padding=(2,))\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0957]), zero_point=tensor([80]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.1404377818107605, zero_point=62, padding=(2,))\n",
       "            )\n",
       "            (3): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1023]), zero_point=tensor([79]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.10726557672023773, zero_point=60, padding=(2,))\n",
       "            )\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0156]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.03527877852320671, zero_point=63)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0156]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.0393698625266552, zero_point=67)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0154]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.03884327411651611, zero_point=68)\n",
       "            )\n",
       "            (3): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.01699688285589218, zero_point=60)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0159]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 1536, kernel_size=(1,), stride=(1,), scale=0.03497122600674629, zero_point=70)\n",
       "          )\n",
       "        )\n",
       "        (post): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0364]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 96, kernel_size=(1,), stride=(1,), scale=0.09219082444906235, zero_point=69)\n",
       "        )\n",
       "      )\n",
       "      (5): Flip()\n",
       "      (6): ResidualCouplingTransformersLayer2(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0893]), zero_point=tensor([57]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(96, 192, kernel_size=(1,), stride=(1,), scale=0.058463938534259796, zero_point=67)\n",
       "        )\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0570]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.11191049963235855, zero_point=61)\n",
       "              )\n",
       "              (conv_k): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0570]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.08408331871032715, zero_point=68)\n",
       "              )\n",
       "              (conv_v): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0570]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.09648043662309647, zero_point=69)\n",
       "              )\n",
       "              (conv_o): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0375]), zero_point=tensor([75]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.1798783242702484, zero_point=66)\n",
       "              )\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0239]), zero_point=tensor([67]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(5,), stride=(1,), scale=0.11828681826591492, zero_point=67)\n",
       "              )\n",
       "              (conv_2): QuantWrapper(\n",
       "                (quant): Quantize(scale=tensor([0.0550]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "                (dequant): DeQuantize()\n",
       "                (module): QuantizedConv1d(192, 192, kernel_size=(5,), stride=(1,), scale=0.1229393407702446, zero_point=55)\n",
       "              )\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0678]), zero_point=tensor([68]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.20143917202949524, zero_point=65, padding=(2,))\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0722]), zero_point=tensor([69]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.2161848396062851, zero_point=70, padding=(2,))\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0792]), zero_point=tensor([71]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.17042504251003265, zero_point=71, padding=(2,))\n",
       "            )\n",
       "            (3): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0878]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(5,), stride=(1,), scale=0.15224865078926086, zero_point=71, padding=(2,))\n",
       "            )\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0156]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.03507715463638306, zero_point=65)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.04354032874107361, zero_point=74)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0155]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 384, kernel_size=(1,), stride=(1,), scale=0.042402248829603195, zero_point=66)\n",
       "            )\n",
       "            (3): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0156]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(192, 192, kernel_size=(1,), stride=(1,), scale=0.012616540305316448, zero_point=70)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0159]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 1536, kernel_size=(1,), stride=(1,), scale=0.04007265716791153, zero_point=57)\n",
       "          )\n",
       "        )\n",
       "        (post): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0273]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(192, 96, kernel_size=(1,), stride=(1,), scale=0.09146449714899063, zero_point=69)\n",
       "        )\n",
       "      )\n",
       "      (7): Flip()\n",
       "    )\n",
       "  )\n",
       "  (dp): StochasticDurationPredictor(\n",
       "    (log_flow): Log()\n",
       "    (flows): ModuleList(\n",
       "      (0): ElementwiseAffine()\n",
       "      (1): ConvFlow(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0441]), zero_point=tensor([86]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(1, 256, kernel_size=(1,), stride=(1,), scale=0.05988822504878044, zero_point=63)\n",
       "        )\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0805]), zero_point=tensor([70]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.048246271908283234, zero_point=74, padding=(1,), groups=256)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0890]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.05426705628633499, zero_point=66, padding=(3,), dilation=(3,), groups=256)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0921]), zero_point=tensor([60]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.047318488359451294, zero_point=64, padding=(9,), dilation=(9,), groups=256)\n",
       "            )\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0319]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.0582299530506134, zero_point=65)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0370]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.0507727712392807, zero_point=58)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0390]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.07549791783094406, zero_point=61)\n",
       "            )\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0885]), zero_point=tensor([49]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 29, kernel_size=(1,), stride=(1,), scale=0.14800360798835754, zero_point=82)\n",
       "        )\n",
       "      )\n",
       "      (2): Flip()\n",
       "      (3): ConvFlow(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0328]), zero_point=tensor([82]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(1, 256, kernel_size=(1,), stride=(1,), scale=0.03852896764874458, zero_point=60)\n",
       "        )\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0665]), zero_point=tensor([67]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.03485814854502678, zero_point=69, padding=(1,), groups=256)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0716]), zero_point=tensor([61]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.04092833027243614, zero_point=61, padding=(3,), dilation=(3,), groups=256)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0838]), zero_point=tensor([53]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.05542342737317085, zero_point=68, padding=(9,), dilation=(9,), groups=256)\n",
       "            )\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0405]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.04149385169148445, zero_point=54)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0460]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.03939758986234665, zero_point=48)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0443]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.039766453206539154, zero_point=48)\n",
       "            )\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1127]), zero_point=tensor([36]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 29, kernel_size=(1,), stride=(1,), scale=0.23917356133460999, zero_point=65)\n",
       "        )\n",
       "      )\n",
       "      (4): Flip()\n",
       "      (5): ConvFlow(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0531]), zero_point=tensor([67]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(1, 256, kernel_size=(1,), stride=(1,), scale=0.05473015829920769, zero_point=63)\n",
       "        )\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0794]), zero_point=tensor([65]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.045005377382040024, zero_point=71, padding=(1,), groups=256)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0877]), zero_point=tensor([60]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.09606406837701797, zero_point=92, padding=(3,), dilation=(3,), groups=256)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0940]), zero_point=tensor([57]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.09493488073348999, zero_point=89, padding=(9,), dilation=(9,), groups=256)\n",
       "            )\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0306]), zero_point=tensor([6]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.0494229681789875, zero_point=56)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0340]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.04066852480173111, zero_point=50)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0342]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.05438118427991867, zero_point=62)\n",
       "            )\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1058]), zero_point=tensor([52]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 29, kernel_size=(1,), stride=(1,), scale=0.2940289080142975, zero_point=82)\n",
       "        )\n",
       "      )\n",
       "      (6): Flip()\n",
       "      (7): ConvFlow(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0524]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(1, 256, kernel_size=(1,), stride=(1,), scale=0.050816018134355545, zero_point=64)\n",
       "        )\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0806]), zero_point=tensor([60]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.042487893253564835, zero_point=66, padding=(1,), groups=256)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0889]), zero_point=tensor([56]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.05458671227097511, zero_point=57, padding=(3,), dilation=(3,), groups=256)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0947]), zero_point=tensor([54]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.05238717049360275, zero_point=59, padding=(9,), dilation=(9,), groups=256)\n",
       "            )\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0352]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.053505945950746536, zero_point=60)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0447]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.05605572834610939, zero_point=59)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0486]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.08664432913064957, zero_point=73)\n",
       "            )\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1100]), zero_point=tensor([46]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 29, kernel_size=(1,), stride=(1,), scale=0.30414414405822754, zero_point=63)\n",
       "        )\n",
       "      )\n",
       "      (8): Flip()\n",
       "    )\n",
       "    (post_pre): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.1155]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(1, 256, kernel_size=(1,), stride=(1,), scale=0.11453893780708313, zero_point=67)\n",
       "    )\n",
       "    (post_proj): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.0924]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.07309545576572418, zero_point=91)\n",
       "    )\n",
       "    (post_convs): DDSConv(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (convs_sep): ModuleList(\n",
       "        (0): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1043]), zero_point=tensor([67]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.05587305873632431, zero_point=67, padding=(1,), groups=256)\n",
       "        )\n",
       "        (1): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1028]), zero_point=tensor([67]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.04937852546572685, zero_point=67, padding=(3,), dilation=(3,), groups=256)\n",
       "        )\n",
       "        (2): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0967]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.061082419008016586, zero_point=69, padding=(9,), dilation=(9,), groups=256)\n",
       "        )\n",
       "      )\n",
       "      (convs_1x1): ModuleList(\n",
       "        (0): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0292]), zero_point=tensor([6]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.14526695013046265, zero_point=91)\n",
       "        )\n",
       "        (1): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0353]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.20135976374149323, zero_point=94)\n",
       "        )\n",
       "        (2): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0531]), zero_point=tensor([3]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.1572357714176178, zero_point=99)\n",
       "        )\n",
       "      )\n",
       "      (norms_1): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "      (norms_2): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (post_flows): ModuleList(\n",
       "      (0): ElementwiseAffine()\n",
       "      (1): ConvFlow(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1044]), zero_point=tensor([62]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(1, 256, kernel_size=(1,), stride=(1,), scale=0.07423467189073563, zero_point=60)\n",
       "        )\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1063]), zero_point=tensor([67]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.050896935164928436, zero_point=71, padding=(1,), groups=256)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1022]), zero_point=tensor([61]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.0626162737607956, zero_point=74, padding=(3,), dilation=(3,), groups=256)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0985]), zero_point=tensor([60]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.054283834993839264, zero_point=66, padding=(9,), dilation=(9,), groups=256)\n",
       "            )\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0326]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.055851202458143234, zero_point=63)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0347]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.06593528389930725, zero_point=68)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0476]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.05799373984336853, zero_point=64)\n",
       "            )\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1189]), zero_point=tensor([49]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 29, kernel_size=(1,), stride=(1,), scale=0.10098119080066681, zero_point=66)\n",
       "        )\n",
       "      )\n",
       "      (2): Flip()\n",
       "      (3): ConvFlow(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0414]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(1, 256, kernel_size=(1,), stride=(1,), scale=0.04799631983041763, zero_point=66)\n",
       "        )\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0874]), zero_point=tensor([69]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.04837530106306076, zero_point=57, padding=(1,), groups=256)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0868]), zero_point=tensor([67]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.0869319960474968, zero_point=91, padding=(3,), dilation=(3,), groups=256)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0845]), zero_point=tensor([66]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.05773739516735077, zero_point=73, padding=(9,), dilation=(9,), groups=256)\n",
       "            )\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0385]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.0455821193754673, zero_point=57)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0365]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.046722568571567535, zero_point=71)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0379]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.06473226100206375, zero_point=68)\n",
       "            )\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0834]), zero_point=tensor([58]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 29, kernel_size=(1,), stride=(1,), scale=0.19616740942001343, zero_point=74)\n",
       "        )\n",
       "      )\n",
       "      (4): Flip()\n",
       "      (5): ConvFlow(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.1044]), zero_point=tensor([62]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(1, 256, kernel_size=(1,), stride=(1,), scale=0.08168286830186844, zero_point=65)\n",
       "        )\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.1026]), zero_point=tensor([73]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.05511074885725975, zero_point=74, padding=(1,), groups=256)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0972]), zero_point=tensor([69]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.057064976543188095, zero_point=76, padding=(3,), dilation=(3,), groups=256)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0954]), zero_point=tensor([65]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.05039391666650772, zero_point=68, padding=(9,), dilation=(9,), groups=256)\n",
       "            )\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0312]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.04777446761727333, zero_point=59)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0381]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.05960031598806381, zero_point=72)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0368]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.07067641615867615, zero_point=61)\n",
       "            )\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0978]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 29, kernel_size=(1,), stride=(1,), scale=0.10610667616128922, zero_point=81)\n",
       "        )\n",
       "      )\n",
       "      (6): Flip()\n",
       "      (7): ConvFlow(\n",
       "        (pre): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0384]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(1, 256, kernel_size=(1,), stride=(1,), scale=0.04096485674381256, zero_point=59)\n",
       "        )\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0909]), zero_point=tensor([66]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.05239567905664444, zero_point=63, padding=(1,), groups=256)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0929]), zero_point=tensor([65]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.08087601512670517, zero_point=82, padding=(3,), dilation=(3,), groups=256)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0914]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.08767401427030563, zero_point=78, padding=(9,), dilation=(9,), groups=256)\n",
       "            )\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0350]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.04588991403579712, zero_point=54)\n",
       "            )\n",
       "            (1): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0432]), zero_point=tensor([4]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.04263437166810036, zero_point=58)\n",
       "            )\n",
       "            (2): QuantWrapper(\n",
       "              (quant): Quantize(scale=tensor([0.0508]), zero_point=tensor([3]), dtype=torch.quint8)\n",
       "              (dequant): DeQuantize()\n",
       "              (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.06129450350999832, zero_point=65)\n",
       "            )\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0920]), zero_point=tensor([61]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 29, kernel_size=(1,), stride=(1,), scale=0.1948220282793045, zero_point=69)\n",
       "        )\n",
       "      )\n",
       "      (8): Flip()\n",
       "    )\n",
       "    (pre): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.0153]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(192, 256, kernel_size=(1,), stride=(1,), scale=0.01475522294640541, zero_point=66)\n",
       "    )\n",
       "    (proj): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.0408]), zero_point=tensor([38]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.06909027695655823, zero_point=70)\n",
       "    )\n",
       "    (convs): DDSConv(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (convs_sep): ModuleList(\n",
       "        (0): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0185]), zero_point=tensor([65]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.008624116890132427, zero_point=55, padding=(1,), groups=256)\n",
       "        )\n",
       "        (1): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0332]), zero_point=tensor([41]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.019773907959461212, zero_point=62, padding=(3,), dilation=(3,), groups=256)\n",
       "        )\n",
       "        (2): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0393]), zero_point=tensor([37]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.028371835127472878, zero_point=71, padding=(9,), dilation=(9,), groups=256)\n",
       "        )\n",
       "      )\n",
       "      (convs_1x1): ModuleList(\n",
       "        (0): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0351]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.06722318381071091, zero_point=75)\n",
       "        )\n",
       "        (1): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0321]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.09294761717319489, zero_point=97)\n",
       "        )\n",
       "        (2): QuantWrapper(\n",
       "          (quant): Quantize(scale=tensor([0.0362]), zero_point=tensor([5]), dtype=torch.quint8)\n",
       "          (dequant): DeQuantize()\n",
       "          (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.1588115394115448, zero_point=108)\n",
       "        )\n",
       "      )\n",
       "      (norms_1): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "      (norms_2): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (cond): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.0159]), zero_point=tensor([63]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(256, 256, kernel_size=(1,), stride=(1,), scale=0.01387436967343092, zero_point=57)\n",
       "    )\n",
       "  )\n",
       "  (emb_g): Embedding(5, 256)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"db-finetune/out/G_QAT_260.pth\", map_location=device)\n",
    "missing, unexpected = net_g.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "net_g.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "978a2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ' ,   +  .   ,  ,   + --  .'\n",
    "out = 'congrats_qat_quanted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ba5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(txt, config):\n",
    "    text_norm = text.text_to_sequence_g2p(txt)\n",
    "    if config['data']['add_blank']:\n",
    "        text_norm = commons.intersperse(text_norm, 0)\n",
    "    text_norm = torch.LongTensor(text_norm)\n",
    "    print(text_norm)\n",
    "    return text_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a6fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vcss(out, inputstr, i):  # single\n",
    "    device = torch.device(\"cpu\")  # : quantized  = CPU\n",
    "    net_g.to(device)\n",
    "    net_g.eval()\n",
    "\n",
    "    stn_tst = get_text(inputstr, config)\n",
    "\n",
    "    speed = 1.0\n",
    "    output_dir = r'outputs'\n",
    "    sid = torch.LongTensor([i]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_tst = stn_tst.to(device).unsqueeze(0)\n",
    "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "\n",
    "        o, o_mb, *_ = net_g.infer(\n",
    "            x_tst,\n",
    "            x_tst_lengths,\n",
    "            sid=sid,\n",
    "            noise_scale=.667,\n",
    "            noise_scale_w=0.8,\n",
    "            length_scale=1 / speed,\n",
    "        )\n",
    "\n",
    "        audio = o[0, 0].cpu().numpy() * 32768.0  # vol scale\n",
    "\n",
    "    write(rf'{output_dir}/{out}.wav', config['data']['sampling_rate'], audio.astype(np.int16))\n",
    "    print(rf'{out}.wav Generated!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "682be7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.weight_norm import WeightNorm, remove_weight_norm\n",
    "\n",
    "\n",
    "def strip_weight_norm_hooks(model):\n",
    "    for m in model.modules():\n",
    "        # Try to remove weight_norm parametrization if present\n",
    "        try:\n",
    "            remove_weight_norm(m)\n",
    "        except (ValueError, AttributeError):\n",
    "            pass\n",
    "        # Drop any leftover WeightNorm forward pre-hooks (quantized convs have no weight_g)\n",
    "        if hasattr(m, \"_forward_pre_hooks\"):\n",
    "            for k, hook in list(m._forward_pre_hooks.items()):\n",
    "                if isinstance(hook, WeightNorm):\n",
    "                    del m._forward_pre_hooks[k]\n",
    "\n",
    "\n",
    "# strip_weight_norm_hooks(net_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "578ccead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 37,  0, 41,  0, 21,  0, 23,  0, 36,  0,\n",
      "         8,  0,  3,  0, 59,  0, 14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0,\n",
      "        39,  0, 39,  0, 14,  0, 32,  0, 14,  0,  3,  0, 31,  0, 59,  0,  3,  0,\n",
      "        19,  0, 22,  0, 33,  0, 43,  0, 42,  0, 30,  0, 39,  0, 51,  0, 14,  0,\n",
      "         3,  0, 43,  0, 41,  0, 47,  0, 36,  0, 23,  0,  3,  0, 33,  0, 55,  0,\n",
      "        14,  0, 39,  0, 52,  0, 30,  0, 59,  0, 15,  0, 18,  0, 30,  0, 30,  0,\n",
      "        10,  0,  3,  0, 32,  0, 23,  0, 47,  0, 36,  0, 30,  0,  3,  0, 38,  0,\n",
      "        22,  0, 40,  0, 15,  0,  3,  0, 47,  0, 35,  0, 58,  0, 49,  0, 39,  0,\n",
      "        41,  0,  8,  0,  3,  0, 51,  0, 41,  0,  3,  0, 51,  0, 42,  0, 51,  0,\n",
      "         8,  0,  3,  0, 33,  0, 51,  0, 42,  0,  3,  0, 38,  0, 22,  0, 40,  0,\n",
      "        15,  0,  3,  0, 33,  0, 55,  0, 14,  0, 39,  0, 52,  0, 30,  0, 59,  0,\n",
      "        41,  0, 55,  0, 15,  0, 35,  0,  3,  0,  3,  0, 20,  0, 41,  0, 47,  0,\n",
      "        51,  0, 42,  0, 30,  0, 39,  0,  3,  0, 53,  0, 55,  0, 14,  0, 60,  0,\n",
      "        23,  0, 40,  0, 30,  0, 32,  0, 14,  0, 10,  0,  2])\n",
      "congrats_qat_quanted.wav Generated!\n"
     ]
    }
   ],
   "source": [
    "vcss(out, txt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a212afb",
   "metadata": {},
   "source": [
    ",    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f4c00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "837edf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q MODULE: enc_p.encoder.attn_layers.0.conv_q.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.0.conv_k.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.0.conv_v.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.0.conv_o.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.1.conv_q.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.1.conv_k.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.1.conv_v.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.1.conv_o.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.2.conv_q.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.2.conv_k.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.2.conv_v.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.2.conv_o.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.3.conv_q.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.3.conv_k.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.3.conv_v.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.3.conv_o.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.4.conv_q.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.4.conv_k.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.4.conv_v.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.4.conv_o.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.5.conv_q.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.5.conv_k.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.5.conv_v.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.attn_layers.5.conv_o.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.0.conv_1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.0.conv_2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.1.conv_1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.1.conv_2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.2.conv_1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.2.conv_2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.3.conv_1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.3.conv_2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.4.conv_1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.4.conv_2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.5.conv_1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.ffn_layers.5.conv_2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_p.encoder.spk_emb_linear.module -> <class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "Q MODULE: enc_p.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.conv_pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.ups.0.module -> <class 'torch.ao.nn.quantized.modules.conv.ConvTranspose1d'>\n",
      "Q MODULE: dec.ups.1.module -> <class 'torch.ao.nn.quantized.modules.conv.ConvTranspose1d'>\n",
      "Q MODULE: dec.resblocks.0.convs1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.0.convs1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.0.convs1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.0.convs2.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.0.convs2.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.0.convs2.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.1.convs1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.1.convs1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.1.convs1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.1.convs2.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.1.convs2.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.1.convs2.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.2.convs1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.2.convs1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.2.convs1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.2.convs2.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.2.convs2.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.2.convs2.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.3.convs1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.3.convs1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.3.convs1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.3.convs2.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.3.convs2.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.3.convs2.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.4.convs1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.4.convs1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.4.convs1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.4.convs2.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.4.convs2.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.4.convs2.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.5.convs1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.5.convs1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.5.convs1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.5.convs2.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.5.convs2.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.resblocks.5.convs2.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dec.subband_conv_post.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.3.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.4.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.5.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.6.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.7.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.8.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.9.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.10.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.11.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.12.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.13.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.14.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.in_layers.15.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.3.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.4.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.5.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.6.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.7.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.8.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.9.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.10.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.11.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.12.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.13.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.14.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.res_skip_layers.15.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.enc.cond_layer.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: enc_q.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.pre_transformer.attn_layers.0.conv_q.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.pre_transformer.attn_layers.0.conv_k.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.pre_transformer.attn_layers.0.conv_v.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.pre_transformer.attn_layers.0.conv_o.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.pre_transformer.ffn_layers.0.conv_1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.pre_transformer.ffn_layers.0.conv_2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.enc.in_layers.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.enc.in_layers.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.enc.in_layers.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.enc.in_layers.3.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.enc.res_skip_layers.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.enc.res_skip_layers.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.enc.res_skip_layers.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.enc.res_skip_layers.3.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.enc.cond_layer.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.0.post.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.pre_transformer.attn_layers.0.conv_q.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.pre_transformer.attn_layers.0.conv_k.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.pre_transformer.attn_layers.0.conv_v.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.pre_transformer.attn_layers.0.conv_o.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.pre_transformer.ffn_layers.0.conv_1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.pre_transformer.ffn_layers.0.conv_2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.enc.in_layers.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.enc.in_layers.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.enc.in_layers.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.enc.in_layers.3.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.enc.res_skip_layers.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.enc.res_skip_layers.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.enc.res_skip_layers.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.enc.res_skip_layers.3.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.enc.cond_layer.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.2.post.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.pre_transformer.attn_layers.0.conv_q.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.pre_transformer.attn_layers.0.conv_k.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.pre_transformer.attn_layers.0.conv_v.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.pre_transformer.attn_layers.0.conv_o.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.pre_transformer.ffn_layers.0.conv_1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.pre_transformer.ffn_layers.0.conv_2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.enc.in_layers.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.enc.in_layers.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.enc.in_layers.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.enc.in_layers.3.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.enc.res_skip_layers.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.enc.res_skip_layers.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.enc.res_skip_layers.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.enc.res_skip_layers.3.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.enc.cond_layer.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.4.post.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.pre_transformer.attn_layers.0.conv_q.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.pre_transformer.attn_layers.0.conv_k.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.pre_transformer.attn_layers.0.conv_v.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.pre_transformer.attn_layers.0.conv_o.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.pre_transformer.ffn_layers.0.conv_1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.pre_transformer.ffn_layers.0.conv_2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.enc.in_layers.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.enc.in_layers.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.enc.in_layers.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.enc.in_layers.3.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.enc.res_skip_layers.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.enc.res_skip_layers.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.enc.res_skip_layers.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.enc.res_skip_layers.3.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.enc.cond_layer.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: flow.flows.6.post.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.1.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.1.convs.convs_sep.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.1.convs.convs_sep.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.1.convs.convs_sep.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.1.convs.convs_1x1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.1.convs.convs_1x1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.1.convs.convs_1x1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.1.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.3.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.3.convs.convs_sep.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.3.convs.convs_sep.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.3.convs.convs_sep.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.3.convs.convs_1x1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.3.convs.convs_1x1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.3.convs.convs_1x1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.3.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.5.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.5.convs.convs_sep.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.5.convs.convs_sep.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.5.convs.convs_sep.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.5.convs.convs_1x1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.5.convs.convs_1x1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.5.convs.convs_1x1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.5.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.7.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.7.convs.convs_sep.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.7.convs.convs_sep.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.7.convs.convs_sep.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.7.convs.convs_1x1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.7.convs.convs_1x1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.7.convs.convs_1x1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.flows.7.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_convs.convs_sep.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_convs.convs_sep.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_convs.convs_sep.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_convs.convs_1x1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_convs.convs_1x1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_convs.convs_1x1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.1.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.1.convs.convs_sep.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.1.convs.convs_sep.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.1.convs.convs_sep.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.1.convs.convs_1x1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.1.convs.convs_1x1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.1.convs.convs_1x1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.1.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.3.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.3.convs.convs_sep.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.3.convs.convs_sep.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.3.convs.convs_sep.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.3.convs.convs_1x1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.3.convs.convs_1x1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.3.convs.convs_1x1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.3.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.5.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.5.convs.convs_sep.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.5.convs.convs_sep.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.5.convs.convs_sep.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.5.convs.convs_1x1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.5.convs.convs_1x1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.5.convs.convs_1x1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.5.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.7.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.7.convs.convs_sep.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.7.convs.convs_sep.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.7.convs.convs_sep.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.7.convs.convs_1x1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.7.convs.convs_1x1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.7.convs.convs_1x1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.post_flows.7.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.pre.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.proj.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.convs.convs_sep.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.convs.convs_sep.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.convs.convs_sep.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.convs.convs_1x1.0.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.convs.convs_1x1.1.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.convs.convs_1x1.2.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n",
      "Q MODULE: dp.cond.module -> <class 'torch.ao.nn.quantized.modules.conv.Conv1d'>\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.quantized as nnq\n",
    "\n",
    "def print_quantized_modules(model):\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nnq.Conv1d, nnq.Conv2d, nnq.Conv3d,\n",
    "                          nnq.ConvTranspose1d, nnq.ConvTranspose2d,\n",
    "                          nnq.Linear)):\n",
    "            print(\"Q MODULE:\", name, \"->\", type(m))\n",
    "\n",
    "print_quantized_modules(net_g)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
