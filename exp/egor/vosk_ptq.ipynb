{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:35:54.175485Z",
     "start_time": "2025-11-09T19:35:54.159749Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "vosk_path = r'../../vosk-tts/training/vits2'\n",
    "sys.path.append(vosk_path)\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d60d9192141c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:36:08.413937Z",
     "start_time": "2025-11-09T19:35:54.596758Z"
    }
   },
   "outputs": [],
   "source": [
    "import models\n",
    "import text\n",
    "import utils\n",
    "import data_utils\n",
    "import json\n",
    "import commons\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f150e9c983a7416f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:36:08.440546Z",
     "start_time": "2025-11-09T19:36:08.413937Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(r'../../pretrained/config.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377eb430cd2d7134",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:36:08.497820Z",
     "start_time": "2025-11-09T19:36:08.475823Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f753562f7efb41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:36:08.545497Z",
     "start_time": "2025-11-09T19:36:08.529739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'log_interval': 200,\n",
       "  'eval_interval': 1000,\n",
       "  'seed': 1234,\n",
       "  'epochs': 20000,\n",
       "  'learning_rate': 0.0002,\n",
       "  'betas': [0.8, 0.99],\n",
       "  'eps': 1e-09,\n",
       "  'batch_size': 24,\n",
       "  'fp16_run': False,\n",
       "  'lr_decay': 0.999875,\n",
       "  'segment_size': 8192,\n",
       "  'init_lr_ratio': 1,\n",
       "  'warmup_epochs': 0,\n",
       "  'c_mel': 45,\n",
       "  'c_kl': 1.0,\n",
       "  'fft_sizes': [384, 683, 171],\n",
       "  'hop_sizes': [30, 60, 10],\n",
       "  'win_lengths': [150, 300, 60],\n",
       "  'window': 'hann_window'},\n",
       " 'data': {'use_mel_posterior_encoder': True,\n",
       "  'training_files': 'db/metadata-phones-ids.csv.train',\n",
       "  'validation_files': 'db/metadata-phones-ids.csv.dev',\n",
       "  'text_cleaners': [''],\n",
       "  'max_wav_value': 32768.0,\n",
       "  'sampling_rate': 22050,\n",
       "  'filter_length': 1024,\n",
       "  'hop_length': 256,\n",
       "  'win_length': 1024,\n",
       "  'n_mel_channels': 80,\n",
       "  'mel_fmin': 0.0,\n",
       "  'mel_fmax': None,\n",
       "  'add_blank': True,\n",
       "  'n_speakers': 5,\n",
       "  'cleaned_text': False,\n",
       "  'g2p_text': False,\n",
       "  'aligned_text': True},\n",
       " 'model': {'use_mel_posterior_encoder': True,\n",
       "  'use_transformer_flows': True,\n",
       "  'transformer_flow_type': 'pre_conv2',\n",
       "  'use_spk_conditioned_encoder': True,\n",
       "  'use_noise_scaled_mas': True,\n",
       "  'use_duration_discriminator': True,\n",
       "  'duration_discriminator_type': 'dur_disc_2',\n",
       "  'ms_istft_vits': False,\n",
       "  'mb_istft_vits': True,\n",
       "  'istft_vits': False,\n",
       "  'subbands': 4,\n",
       "  'gen_istft_n_fft': 16,\n",
       "  'gen_istft_hop_size': 4,\n",
       "  'inter_channels': 192,\n",
       "  'hidden_channels': 192,\n",
       "  'filter_channels': 768,\n",
       "  'n_heads': 2,\n",
       "  'n_layers': 6,\n",
       "  'kernel_size': 3,\n",
       "  'p_dropout': 0.1,\n",
       "  'resblock': '1',\n",
       "  'resblock_kernel_sizes': [3, 7, 11],\n",
       "  'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
       "  'upsample_rates': [4, 4],\n",
       "  'upsample_initial_channel': 512,\n",
       "  'upsample_kernel_sizes': [16, 16],\n",
       "  'n_layers_q': 3,\n",
       "  'use_spectral_norm': False,\n",
       "  'use_sdp': True,\n",
       "  'gin_channels': 256}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d5d4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.8.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /Users/egorkolesnikv/Documents/ai_talent/edl/phone-tts/.venv/lib/python3.9/site-packages\n",
      "Requires: sympy, jinja2, fsspec, typing-extensions, filelock, networkx\n",
      "Required-by: torchaudio\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f046af8032857ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:36:09.124730Z",
     "start_time": "2025-11-09T19:36:08.577479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 2\n",
      "Multi-band iSTFT VITS2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egorkolesnikv/Documents/ai_talent/edl/phone-tts/.venv/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "from text.symbols import symbols\n",
    "net_g = models.SynthesizerTrn(\n",
    "    len(symbols),\n",
    "    80,\n",
    "    config['train']['segment_size'] // config['data']['hop_length'],\n",
    "    n_speakers=config['data']['n_speakers'],\n",
    "    mas_noise_scale_initial=0.01,\n",
    "    noise_scale_delta=2e-6,\n",
    "    **config['model']).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b567533a52b0d637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:36:09.501147Z",
     "start_time": "2025-11-09T19:36:09.161093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded checkpoint '../../pretrained/G_1000.pth' (iteration 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SynthesizerTrn(\n",
       "   (enc_p): TextEncoder(\n",
       "     (emb): Embedding(62, 192)\n",
       "     (encoder): Encoder(\n",
       "       (drop): Dropout(p=0.1, inplace=False)\n",
       "       (attn_layers): ModuleList(\n",
       "         (0-5): 6 x MultiHeadAttention(\n",
       "           (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "           (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "           (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "           (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (norm_layers_1): ModuleList(\n",
       "         (0-5): 6 x LayerNorm()\n",
       "       )\n",
       "       (ffn_layers): ModuleList(\n",
       "         (0-5): 6 x FFN(\n",
       "           (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
       "           (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
       "           (drop): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (norm_layers_2): ModuleList(\n",
       "         (0-5): 6 x LayerNorm()\n",
       "       )\n",
       "       (spk_emb_linear): Linear(in_features=256, out_features=192, bias=True)\n",
       "     )\n",
       "     (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "   )\n",
       "   (dec): Multiband_iSTFT_Generator(\n",
       "     (conv_pre): Conv1d(192, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "     (ups): ModuleList(\n",
       "       (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(4,), padding=(6,))\n",
       "       (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(4,), padding=(6,))\n",
       "     )\n",
       "     (resblocks): ModuleList(\n",
       "       (0): ResBlock1(\n",
       "         (convs1): ModuleList(\n",
       "           (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "           (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "           (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "         )\n",
       "         (convs2): ModuleList(\n",
       "           (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "         )\n",
       "       )\n",
       "       (1): ResBlock1(\n",
       "         (convs1): ModuleList(\n",
       "           (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "           (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "           (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "         )\n",
       "         (convs2): ModuleList(\n",
       "           (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "         )\n",
       "       )\n",
       "       (2): ResBlock1(\n",
       "         (convs1): ModuleList(\n",
       "           (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "           (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "           (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "         )\n",
       "         (convs2): ModuleList(\n",
       "           (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "         )\n",
       "       )\n",
       "       (3): ResBlock1(\n",
       "         (convs1): ModuleList(\n",
       "           (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "           (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "           (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "         )\n",
       "         (convs2): ModuleList(\n",
       "           (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "         )\n",
       "       )\n",
       "       (4): ResBlock1(\n",
       "         (convs1): ModuleList(\n",
       "           (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "           (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "           (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "         )\n",
       "         (convs2): ModuleList(\n",
       "           (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "         )\n",
       "       )\n",
       "       (5): ResBlock1(\n",
       "         (convs1): ModuleList(\n",
       "           (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "           (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "           (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "         )\n",
       "         (convs2): ModuleList(\n",
       "           (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (reflection_pad): ReflectionPad1d((1, 0))\n",
       "     (subband_conv_post): Conv1d(128, 72, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "     (stft): TorchSTFT()\n",
       "   )\n",
       "   (enc_q): PosteriorEncoder(\n",
       "     (pre): Conv1d(80, 192, kernel_size=(1,), stride=(1,))\n",
       "     (enc): WN(\n",
       "       (in_layers): ModuleList(\n",
       "         (0-15): 16 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "       (res_skip_layers): ModuleList(\n",
       "         (0-14): 15 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "         (15): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (drop): Dropout(p=0, inplace=False)\n",
       "       (cond_layer): Conv1d(256, 6144, kernel_size=(1,), stride=(1,))\n",
       "     )\n",
       "     (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "   )\n",
       "   (flow): ResidualCouplingTransformersBlock(\n",
       "     (flows): ModuleList(\n",
       "       (0): ResidualCouplingTransformersLayer2(\n",
       "         (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "         (pre_transformer): Encoder(\n",
       "           (drop): Dropout(p=0, inplace=False)\n",
       "           (attn_layers): ModuleList(\n",
       "             (0): MultiHeadAttention(\n",
       "               (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (drop): Dropout(p=0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (norm_layers_1): ModuleList(\n",
       "             (0): LayerNorm()\n",
       "           )\n",
       "           (ffn_layers): ModuleList(\n",
       "             (0): FFN(\n",
       "               (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "               (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "               (drop): Dropout(p=0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (norm_layers_2): ModuleList(\n",
       "             (0): LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (enc): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "             (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (drop): Dropout(p=0, inplace=False)\n",
       "           (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "         (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (1): Flip()\n",
       "       (2): ResidualCouplingTransformersLayer2(\n",
       "         (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "         (pre_transformer): Encoder(\n",
       "           (drop): Dropout(p=0, inplace=False)\n",
       "           (attn_layers): ModuleList(\n",
       "             (0): MultiHeadAttention(\n",
       "               (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (drop): Dropout(p=0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (norm_layers_1): ModuleList(\n",
       "             (0): LayerNorm()\n",
       "           )\n",
       "           (ffn_layers): ModuleList(\n",
       "             (0): FFN(\n",
       "               (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "               (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "               (drop): Dropout(p=0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (norm_layers_2): ModuleList(\n",
       "             (0): LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (enc): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "             (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (drop): Dropout(p=0, inplace=False)\n",
       "           (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "         (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (3): Flip()\n",
       "       (4): ResidualCouplingTransformersLayer2(\n",
       "         (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "         (pre_transformer): Encoder(\n",
       "           (drop): Dropout(p=0, inplace=False)\n",
       "           (attn_layers): ModuleList(\n",
       "             (0): MultiHeadAttention(\n",
       "               (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (drop): Dropout(p=0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (norm_layers_1): ModuleList(\n",
       "             (0): LayerNorm()\n",
       "           )\n",
       "           (ffn_layers): ModuleList(\n",
       "             (0): FFN(\n",
       "               (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "               (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "               (drop): Dropout(p=0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (norm_layers_2): ModuleList(\n",
       "             (0): LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (enc): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "             (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (drop): Dropout(p=0, inplace=False)\n",
       "           (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "         (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (5): Flip()\n",
       "       (6): ResidualCouplingTransformersLayer2(\n",
       "         (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "         (pre_transformer): Encoder(\n",
       "           (drop): Dropout(p=0, inplace=False)\n",
       "           (attn_layers): ModuleList(\n",
       "             (0): MultiHeadAttention(\n",
       "               (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "               (drop): Dropout(p=0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (norm_layers_1): ModuleList(\n",
       "             (0): LayerNorm()\n",
       "           )\n",
       "           (ffn_layers): ModuleList(\n",
       "             (0): FFN(\n",
       "               (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "               (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "               (drop): Dropout(p=0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (norm_layers_2): ModuleList(\n",
       "             (0): LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (enc): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "             (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (drop): Dropout(p=0, inplace=False)\n",
       "           (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "         (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (7): Flip()\n",
       "     )\n",
       "   )\n",
       "   (dp): StochasticDurationPredictor(\n",
       "     (log_flow): Log()\n",
       "     (flows): ModuleList(\n",
       "       (0): ElementwiseAffine()\n",
       "       (1): ConvFlow(\n",
       "         (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "         (convs): DDSConv(\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "           (convs_sep): ModuleList(\n",
       "             (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "             (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "             (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "           )\n",
       "           (convs_1x1): ModuleList(\n",
       "             (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (norms_1): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "           (norms_2): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (2): Flip()\n",
       "       (3): ConvFlow(\n",
       "         (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "         (convs): DDSConv(\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "           (convs_sep): ModuleList(\n",
       "             (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "             (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "             (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "           )\n",
       "           (convs_1x1): ModuleList(\n",
       "             (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (norms_1): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "           (norms_2): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (4): Flip()\n",
       "       (5): ConvFlow(\n",
       "         (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "         (convs): DDSConv(\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "           (convs_sep): ModuleList(\n",
       "             (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "             (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "             (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "           )\n",
       "           (convs_1x1): ModuleList(\n",
       "             (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (norms_1): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "           (norms_2): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (6): Flip()\n",
       "       (7): ConvFlow(\n",
       "         (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "         (convs): DDSConv(\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "           (convs_sep): ModuleList(\n",
       "             (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "             (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "             (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "           )\n",
       "           (convs_1x1): ModuleList(\n",
       "             (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (norms_1): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "           (norms_2): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (8): Flip()\n",
       "     )\n",
       "     (post_pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "     (post_proj): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "     (post_convs): DDSConv(\n",
       "       (drop): Dropout(p=0.1, inplace=False)\n",
       "       (convs_sep): ModuleList(\n",
       "         (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "         (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "         (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "       )\n",
       "       (convs_1x1): ModuleList(\n",
       "         (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (norms_1): ModuleList(\n",
       "         (0-2): 3 x LayerNorm()\n",
       "       )\n",
       "       (norms_2): ModuleList(\n",
       "         (0-2): 3 x LayerNorm()\n",
       "       )\n",
       "     )\n",
       "     (post_flows): ModuleList(\n",
       "       (0): ElementwiseAffine()\n",
       "       (1): ConvFlow(\n",
       "         (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "         (convs): DDSConv(\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "           (convs_sep): ModuleList(\n",
       "             (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "             (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "             (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "           )\n",
       "           (convs_1x1): ModuleList(\n",
       "             (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (norms_1): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "           (norms_2): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (2): Flip()\n",
       "       (3): ConvFlow(\n",
       "         (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "         (convs): DDSConv(\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "           (convs_sep): ModuleList(\n",
       "             (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "             (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "             (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "           )\n",
       "           (convs_1x1): ModuleList(\n",
       "             (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (norms_1): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "           (norms_2): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (4): Flip()\n",
       "       (5): ConvFlow(\n",
       "         (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "         (convs): DDSConv(\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "           (convs_sep): ModuleList(\n",
       "             (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "             (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "             (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "           )\n",
       "           (convs_1x1): ModuleList(\n",
       "             (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (norms_1): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "           (norms_2): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (6): Flip()\n",
       "       (7): ConvFlow(\n",
       "         (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "         (convs): DDSConv(\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "           (convs_sep): ModuleList(\n",
       "             (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "             (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "             (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "           )\n",
       "           (convs_1x1): ModuleList(\n",
       "             (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (norms_1): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "           (norms_2): ModuleList(\n",
       "             (0-2): 3 x LayerNorm()\n",
       "           )\n",
       "         )\n",
       "         (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (8): Flip()\n",
       "     )\n",
       "     (pre): Conv1d(192, 256, kernel_size=(1,), stride=(1,))\n",
       "     (proj): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "     (convs): DDSConv(\n",
       "       (drop): Dropout(p=0.1, inplace=False)\n",
       "       (convs_sep): ModuleList(\n",
       "         (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "         (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "         (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "       )\n",
       "       (convs_1x1): ModuleList(\n",
       "         (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "       )\n",
       "       (norms_1): ModuleList(\n",
       "         (0-2): 3 x LayerNorm()\n",
       "       )\n",
       "       (norms_2): ModuleList(\n",
       "         (0-2): 3 x LayerNorm()\n",
       "       )\n",
       "     )\n",
       "     (cond): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "   )\n",
       "   (emb_g): Embedding(5, 256)\n",
       " ),\n",
       " None,\n",
       " 0.0002,\n",
       " 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.load_checkpoint(r\"../../pretrained/G_1000.pth\",\n",
    "                    net_g,\n",
    "                    None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f277fba261365923",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:36:09.552008Z",
     "start_time": "2025-11-09T19:36:09.520191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SynthesizerTrn(\n",
       "  (enc_p): TextEncoder(\n",
       "    (emb): Embedding(62, 192)\n",
       "    (encoder): Encoder(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (attn_layers): ModuleList(\n",
       "        (0-5): 6 x MultiHeadAttention(\n",
       "          (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layers_1): ModuleList(\n",
       "        (0-5): 6 x LayerNorm()\n",
       "      )\n",
       "      (ffn_layers): ModuleList(\n",
       "        (0-5): 6 x FFN(\n",
       "          (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
       "          (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layers_2): ModuleList(\n",
       "        (0-5): 6 x LayerNorm()\n",
       "      )\n",
       "      (spk_emb_linear): Linear(in_features=256, out_features=192, bias=True)\n",
       "    )\n",
       "    (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (dec): Multiband_iSTFT_Generator(\n",
       "    (conv_pre): Conv1d(192, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (ups): ModuleList(\n",
       "      (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(4,), padding=(6,))\n",
       "      (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(4,), padding=(6,))\n",
       "    )\n",
       "    (resblocks): ModuleList(\n",
       "      (0): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (reflection_pad): ReflectionPad1d((1, 0))\n",
       "    (subband_conv_post): Conv1d(128, 72, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "    (stft): TorchSTFT()\n",
       "  )\n",
       "  (enc_q): PosteriorEncoder(\n",
       "    (pre): Conv1d(80, 192, kernel_size=(1,), stride=(1,))\n",
       "    (enc): WN(\n",
       "      (in_layers): ModuleList(\n",
       "        (0-15): 16 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (res_skip_layers): ModuleList(\n",
       "        (0-14): 15 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "        (15): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "      (cond_layer): Conv1d(256, 6144, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (flow): ResidualCouplingTransformersBlock(\n",
       "    (flows): ModuleList(\n",
       "      (0): ResidualCouplingTransformersLayer2(\n",
       "        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): Flip()\n",
       "      (2): ResidualCouplingTransformersLayer2(\n",
       "        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): Flip()\n",
       "      (4): ResidualCouplingTransformersLayer2(\n",
       "        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (5): Flip()\n",
       "      (6): ResidualCouplingTransformersLayer2(\n",
       "        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (7): Flip()\n",
       "    )\n",
       "  )\n",
       "  (dp): StochasticDurationPredictor(\n",
       "    (log_flow): Log()\n",
       "    (flows): ModuleList(\n",
       "      (0): ElementwiseAffine()\n",
       "      (1): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Flip()\n",
       "      (3): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (4): Flip()\n",
       "      (5): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (6): Flip()\n",
       "      (7): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (8): Flip()\n",
       "    )\n",
       "    (post_pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "    (post_proj): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (post_convs): DDSConv(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (convs_sep): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "      )\n",
       "      (convs_1x1): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (norms_1): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "      (norms_2): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (post_flows): ModuleList(\n",
       "      (0): ElementwiseAffine()\n",
       "      (1): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Flip()\n",
       "      (3): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (4): Flip()\n",
       "      (5): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (6): Flip()\n",
       "      (7): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (8): Flip()\n",
       "    )\n",
       "    (pre): Conv1d(192, 256, kernel_size=(1,), stride=(1,))\n",
       "    (proj): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (convs): DDSConv(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (convs_sep): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "      )\n",
       "      (convs_1x1): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (norms_1): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "      (norms_2): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (cond): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (emb_g): Embedding(5, 256)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_g.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a48e11cbac0482",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:36:09.588110Z",
     "start_time": "2025-11-09T19:36:09.572432Z"
    }
   },
   "outputs": [],
   "source": [
    "txt = '  + ++,   +.    -  +++  .'\n",
    "out = 'congrats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a11993bd0dbeed10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:36:09.635532Z",
     "start_time": "2025-11-09T19:36:09.619852Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_text(txt, config):\n",
    "    text_norm = text.text_to_sequence_g2p(txt)\n",
    "    if config['data']['add_blank']:\n",
    "        text_norm = commons.intersperse(text_norm, 0)\n",
    "    text_norm = torch.LongTensor(text_norm)\n",
    "    print(text_norm)\n",
    "    return text_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09cf306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
       "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
       "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
       "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
       "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
       "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
       "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
       "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
       "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
       "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
       "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
       "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
       "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
       "        10,  0,  2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(txt, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52b45dd47d83ef18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:36:09.683202Z",
     "start_time": "2025-11-09T19:36:09.668825Z"
    }
   },
   "outputs": [],
   "source": [
    "def vcss(out, inputstr, i):  # single\n",
    "    device = torch.device(\"cpu\")  # : quantized  = CPU\n",
    "    net_g.to(device)\n",
    "    net_g.eval()\n",
    "\n",
    "    stn_tst = get_text(inputstr, config)\n",
    "\n",
    "    speed = 1.0\n",
    "    output_dir = r'outputs'\n",
    "    sid = torch.LongTensor([i]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_tst = stn_tst.to(device).unsqueeze(0)\n",
    "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "\n",
    "        o, o_mb, *_ = net_g.infer(\n",
    "            x_tst,\n",
    "            x_tst_lengths,\n",
    "            sid=sid,\n",
    "            noise_scale=.667,\n",
    "            noise_scale_w=0.8,\n",
    "            length_scale=1 / speed,\n",
    "        )\n",
    "\n",
    "        audio = o[0, 0].cpu().numpy() * 32768.0  # vol scale\n",
    "\n",
    "    write(rf'{output_dir}/{out}.wav', config['data']['sampling_rate'], audio.astype(np.int16))\n",
    "    print(rf'{out}.wav Generated!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51336d7bb449c911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "congrats.wav Generated!\n"
     ]
    }
   ],
   "source": [
    "vcss(out, txt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e506db8",
   "metadata": {},
   "source": [
    "Post-training quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97465a85",
   "metadata": {},
   "source": [
    "    ptq.py,    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82194be2",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5530f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bed45c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MARKS_PATH = f\"{os.getenv('HOME')}/natasha_dataset/marks.txt\" \n",
    "\n",
    "def load_texts_from_marks(marks_path):\n",
    "    texts = []\n",
    "    with open(marks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                _, text = line.split(\"|\", maxsplit=1)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            texts.append(text)\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4360d235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 texts for calibration\n",
      " + + +  + +.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts_for_calib = load_texts_from_marks(MARKS_PATH)\n",
    "print(f\"Loaded {len(texts_for_calib)} texts for calibration\")\n",
    "print(texts_for_calib[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7e202",
   "metadata": {},
   "source": [
    "Dataset   ,  ,    ,    ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e446ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"calib_texts\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()  #  stdout\n",
    "    fmt = logging.Formatter(\"[%(levelname)s] %(name)s: %(message)s\")\n",
    "    handler.setFormatter(fmt)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# from hparams import hps  # ,    \n",
    "class TextOnlyCalibrationDataset(Dataset):\n",
    "    \"\"\"\n",
    "      TextAudioSpeakerLoader:\n",
    "    -   audiopaths_sid_text\n",
    "    -    -\n",
    "       /.\n",
    "    \"\"\"\n",
    "    def __init__(self, filelist_path: str, hparams, logger=None, log_every: int = 1):\n",
    "        #  \"\" ,  :\n",
    "        #  -  \n",
    "        #  -    \n",
    "        #  -  text_cleaners, add_blank  ..\n",
    "        self.base = data_utils.TextAudioSpeakerLoader(filelist_path, hparams)\n",
    "        self.logger = logger\n",
    "        self.log_every = log_every\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base.audiopaths_sid_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audiopath, sid, text, cleaned_text = self.base.audiopaths_sid_text[idx]\n",
    "        #     ,    \n",
    "        text_tensor = self.base.get_text(text, cleaned_text)\n",
    "        sid_tensor = self.base.get_sid(sid)\n",
    "\n",
    "        if self.logger is not None and (idx % self.log_every == 0):\n",
    "            #   ,    \n",
    "            short_text = text if len(text) <= 120 else text[:117] + \"...\"\n",
    "            self.logger.debug(\n",
    "                f\"Calib sample idx={idx} sid={sid} wav={audiopath} text={short_text}\"\n",
    "            )\n",
    "        return text_tensor, sid_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0ab7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class TextSpeakerCollate:\n",
    "    def __call__(self, batch):\n",
    "        # batch: list of (text_tensor, sid_tensor)\n",
    "        texts, sids = zip(*batch)\n",
    "        text_lengths = torch.LongTensor([t.size(0) for t in texts])\n",
    "        max_len = int(text_lengths.max().item())\n",
    "\n",
    "        text_padded = torch.zeros(len(texts), max_len, dtype=torch.long)\n",
    "        for i, t in enumerate(texts):\n",
    "            text_padded[i, :t.size(0)] = t\n",
    "\n",
    "        sids = torch.stack(sids).long().view(-1)  # [B]\n",
    "\n",
    "        return text_padded, text_lengths, sids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c04a8b4c56e0fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from scripts.build_audiopaths_sid_texts import FILELIST_PATH\n",
    "\n",
    "hparams = utils.get_hparams_from_file(f\"{os.getenv('HOME')}/pretrained/config.json\")\n",
    "#      aligned_text  false  g2p_text  true\n",
    "\n",
    "hparams.data['aligned_text'] = False\n",
    "hparams.data['g2p_text'] = True\n",
    "calib_dataset = TextOnlyCalibrationDataset(FILELIST_PATH, hparams.data)\n",
    "calib_collate = TextSpeakerCollate()\n",
    "\n",
    "calib_loader = DataLoader(\n",
    "    calib_dataset,\n",
    "    batch_size=8,     #   \n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=calib_collate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839ba8c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874575ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3deb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calibration_fn(model):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i, (x, x_lengths, sid) in enumerate(calib_loader):\n",
    "            if i >= 30:  # ,     \n",
    "                break\n",
    "\n",
    "            x = x.to(\"cpu\")\n",
    "            x_lengths = x_lengths.to(\"cpu\")\n",
    "            sid = sid.to(\"cpu\")\n",
    "\n",
    "            _ = model.infer(\n",
    "                x=x,\n",
    "                x_lengths=x_lengths,\n",
    "                sid=sid,              # multi-speaker \n",
    "                noise_scale=0.667,\n",
    "                length_scale=1.0,\n",
    "                noise_scale_w=1.0,\n",
    "                max_len=None,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7cca81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calibration_fn(model):\n",
    "#     model.eval()\n",
    "#     with torch.inference_mode():\n",
    "#         for i, (x, x_lengths, sid) in enumerate(calib_loader):\n",
    "#             if i >= 30:\n",
    "#                 break\n",
    "#             x = x.to(\"cpu\")\n",
    "#             x_lengths = x_lengths.to(\"cpu\")\n",
    "#             sid = sid.to(\"cpu\")\n",
    "#             #   infer  observers  \n",
    "#             model.infer(x, x_lengths, sid=sid, noise_scale=0.667,\n",
    "#                         length_scale=1.0, noise_scale_w=0.8, max_len=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80369c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ptq import quantize_ptq\n",
    "# #net_g.dec.remove_weight_norm()\n",
    "# net_g.to(\"cpu\")\n",
    "# net_g.eval()\n",
    "\n",
    "# modules_to_quantize = [\n",
    "#     # \"dec\",  #  \n",
    "#     \"enc_p.encoder\",   # self.enc_p.encoder   attention/FFN-\n",
    "#     #\"dp\",              # DurationPredictor  SDP,   \n",
    "#     #\"flow\",            # ResidualCouplingTransformersBlock,       \n",
    "# ]\n",
    "\n",
    "\n",
    "# quantize_ptq(\n",
    "#     net_g,\n",
    "#     module_names=modules_to_quantize,\n",
    "#     calibration_fn=calibration_fn,\n",
    "#     backend=\"fbgemm\",\n",
    "# )\n",
    "\n",
    "# torch.save({\"model\": net_g.state_dict()}, \"G_natasha_quantized_dec.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429fa9a",
   "metadata": {},
   "source": [
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec576bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m#        \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     :\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mquantize_ptq_convs_only\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalibration_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_roots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#  None,     \u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfbgemm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: net_g\u001b[38;5;241m.\u001b[39mstate_dict()}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG_natasha_quantized_conv_only.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/exp/egor/ptq.py:262\u001b[0m, in \u001b[0;36mquantize_ptq_convs_only\u001b[0;34m(model, calibration_fn, module_roots, backend)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mEnd-to-end PTQ, :\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m  -   conv/convT/linear ( QuantWrapper),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m  - . prepare_model_for_ptq_convs_only\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m prepare_model_for_ptq_convs_only(\n\u001b[1;32m    258\u001b[0m     model,\n\u001b[1;32m    259\u001b[0m     module_roots\u001b[38;5;241m=\u001b[39mmodule_roots,\n\u001b[1;32m    260\u001b[0m     backend\u001b[38;5;241m=\u001b[39mbackend,\n\u001b[1;32m    261\u001b[0m )\n\u001b[0;32m--> 262\u001b[0m \u001b[43mcalibrate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalibration_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m convert_model_from_ptq(model)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/exp/egor/ptq.py:214\u001b[0m, in \u001b[0;36mcalibrate_model\u001b[0;34m(model, calibration_fn)\u001b[0m\n\u001b[1;32m    212\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 214\u001b[0m     \u001b[43mcalibration_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m, in \u001b[0;36mcalibration_fn\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      9\u001b[0m x_lengths \u001b[38;5;241m=\u001b[39m x_lengths\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m sid \u001b[38;5;241m=\u001b[39m sid\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# multi-speaker \u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.667\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_scale_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/exp/egor/../../vosk-tts/training/vits2/models.py:1703\u001b[0m, in \u001b[0;36mSynthesizerTrn.infer\u001b[0;34m(self, x, x_lengths, sid, noise_scale, length_scale, noise_scale_w, max_len)\u001b[0m\n\u001b[1;32m   1700\u001b[0m z_p \u001b[38;5;241m=\u001b[39m m_p \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(m_p) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(logs_p) \u001b[38;5;241m*\u001b[39m noise_scale\n\u001b[1;32m   1701\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow(z_p, y_mask, g\u001b[38;5;241m=\u001b[39mg, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1703\u001b[0m o, o_mb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m o, o_mb, attn, y_mask, (z, z_p, m_p, logs_p)\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/exp/egor/../../vosk-tts/training/vits2/models.py:1035\u001b[0m, in \u001b[0;36mMultiband_iSTFT_Generator.forward\u001b[0;34m(self, x, g)\u001b[0m\n\u001b[1;32m   1033\u001b[0m             xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblocks[i \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_kernels \u001b[38;5;241m+\u001b[39m j](x)\n\u001b[1;32m   1034\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1035\u001b[0m             xs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_kernels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m     x \u001b[38;5;241m=\u001b[39m xs \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_kernels\n\u001b[1;32m   1038\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/exp/egor/../../vosk-tts/training/vits2/modules.py:215\u001b[0m, in \u001b[0;36mResBlock1.forward\u001b[0;34m(self, x, x_mask)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     xt \u001b[38;5;241m=\u001b[39m xt \u001b[38;5;241m*\u001b[39m x_mask\n\u001b[0;32m--> 215\u001b[0m xt \u001b[38;5;241m=\u001b[39m \u001b[43mc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m xt \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(xt, LRELU_SLOPE)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/ao/quantization/stubs.py:73\u001b[0m, in \u001b[0;36mQuantWrapper.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     72\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(X)\n\u001b[0;32m---> 73\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(X)\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1879\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1878\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1881\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1882\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1827\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1824\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1825\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1827\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1829\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1830\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1832\u001b[0m     ):\n\u001b[1;32m   1833\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:371\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:366\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    356\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    365\u001b[0m     )\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ptq import quantize_ptq_convs_only\n",
    "\n",
    "net_g.to(\"cpu\")\n",
    "net_g.eval()\n",
    "\n",
    "# :   weight_norm,      !\n",
    "#    mb_istft_vits / ms_istft_vits     remove_weight_norm().\n",
    "# :\n",
    "try:\n",
    "    net_g.dec.remove_weight_norm()\n",
    "except AttributeError:\n",
    "    pass  #        \n",
    "\n",
    "#     :\n",
    "quantize_ptq_convs_only(\n",
    "    net_g,\n",
    "    calibration_fn=calibration_fn,\n",
    "    module_roots=[\"dec\"],  #  None,     \n",
    "    backend=\"fbgemm\",\n",
    ")\n",
    "\n",
    "torch.save({\"model\": net_g.state_dict()}, \"G_natasha_quantized_conv_only.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad065f",
   "metadata": {},
   "source": [
    "#    (,   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa876c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "congrats_q.wav.wav Generated!\n"
     ]
    }
   ],
   "source": [
    "vcss(\"congrats_q.wav\", txt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net_g, \"G_natasha_quantized_dec_full.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a95691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# net_g = torch.load(\"G_natasha_quantized_dec_full.pt\", map_location=\"cpu\", weights_only=False)\n",
    "# net_g.eval()\n",
    "#    net_g.infer(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f11ec3",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6954d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils import remove_weight_norm, WeightNorm\n",
    "\n",
    "# def fix_weight_norm_after_load(model: torch.nn.Module):\n",
    "#     # 1.     weight_norm ,   .\n",
    "#     #       .weight  weight_g / weight_v   .\n",
    "#     for module in model.modules():\n",
    "#         try:\n",
    "#             remove_weight_norm(module)\n",
    "#         except (ValueError, AttributeError):\n",
    "#             # ValueError       weight_norm,\n",
    "#             # AttributeError    / \"\" \n",
    "#             pass\n",
    "\n",
    "#     # 2.      WeightNorm-\n",
    "#     #    (       Conv1d/ConvTranspose1d,\n",
    "#     #      remove_weight_norm   ).\n",
    "#     for module in model.modules():\n",
    "#         if not hasattr(module, \"_forward_pre_hooks\"):\n",
    "#             continue\n",
    "#         for hook_id, hook in list(module._forward_pre_hooks.items()):\n",
    "#             if isinstance(hook, WeightNorm):\n",
    "#                 del module._forward_pre_hooks[hook_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aa7e35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 2\n",
      "Multi-band iSTFT VITS2\n",
      "Removing weight norm...\n",
      "MISSING: []\n",
      "UNEXPECTED: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egorkolesnikv/Documents/ai_talent/edl/phone-tts/.venv/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SynthesizerTrn(\n",
       "  (enc_p): TextEncoder(\n",
       "    (emb): Embedding(62, 192)\n",
       "    (encoder): Encoder(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (attn_layers): ModuleList(\n",
       "        (0-5): 6 x MultiHeadAttention(\n",
       "          (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layers_1): ModuleList(\n",
       "        (0-5): 6 x LayerNorm()\n",
       "      )\n",
       "      (ffn_layers): ModuleList(\n",
       "        (0-5): 6 x FFN(\n",
       "          (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))\n",
       "          (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm_layers_2): ModuleList(\n",
       "        (0-5): 6 x LayerNorm()\n",
       "      )\n",
       "      (spk_emb_linear): Linear(in_features=256, out_features=192, bias=True)\n",
       "    )\n",
       "    (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (dec): Multiband_iSTFT_Generator(\n",
       "    (conv_pre): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.0556]), zero_point=tensor([130]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(192, 512, kernel_size=(7,), stride=(1,), scale=0.06654132157564163, zero_point=116, padding=(3,))\n",
       "    )\n",
       "    (ups): ModuleList(\n",
       "      (0): QuantWrapper(\n",
       "        (quant): Quantize(scale=tensor([0.0412]), zero_point=tensor([26]), dtype=torch.quint8)\n",
       "        (dequant): DeQuantize()\n",
       "        (module): QuantizedConvTranspose1d(512, 256, kernel_size=(16,), stride=(4,), scale=0.030317990109324455, zero_point=147, padding=(6,))\n",
       "      )\n",
       "      (1): QuantWrapper(\n",
       "        (quant): Quantize(scale=tensor([0.0243]), zero_point=tensor([34]), dtype=torch.quint8)\n",
       "        (dequant): DeQuantize()\n",
       "        (module): QuantizedConvTranspose1d(256, 128, kernel_size=(16,), stride=(4,), scale=0.009664828889071941, zero_point=124, padding=(6,))\n",
       "      )\n",
       "    )\n",
       "    (resblocks): ModuleList(\n",
       "      (0): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0148]), zero_point=tensor([33]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.029529383406043053, zero_point=134, padding=(1,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0153]), zero_point=tensor([44]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.0368649885058403, zero_point=149, padding=(3,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0379]), zero_point=tensor([41]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.11710309237241745, zero_point=181, padding=(5,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0167]), zero_point=tensor([30]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.025987811386585236, zero_point=131, padding=(1,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0191]), zero_point=tensor([31]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.09389237314462662, zero_point=170, padding=(1,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0423]), zero_point=tensor([50]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(3,), stride=(1,), scale=0.10749518871307373, zero_point=159, padding=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0148]), zero_point=tensor([33]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.031108727678656578, zero_point=162, padding=(3,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0140]), zero_point=tensor([43]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.03935503587126732, zero_point=179, padding=(9,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0252]), zero_point=tensor([31]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.04329150915145874, zero_point=133, padding=(15,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0128]), zero_point=tensor([41]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.03182528167963028, zero_point=141, padding=(3,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0139]), zero_point=tensor([52]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.043270986527204514, zero_point=143, padding=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0235]), zero_point=tensor([31]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(7,), stride=(1,), scale=0.05914913862943649, zero_point=133, padding=(3,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0148]), zero_point=tensor([33]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.042557112872600555, zero_point=127, padding=(5,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0299]), zero_point=tensor([44]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.05813238024711609, zero_point=167, padding=(15,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0314]), zero_point=tensor([46]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.05982964113354683, zero_point=141, padding=(25,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0227]), zero_point=tensor([30]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.057736411690711975, zero_point=165, padding=(5,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0242]), zero_point=tensor([43]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.06504174321889877, zero_point=164, padding=(5,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0323]), zero_point=tensor([43]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(256, 256, kernel_size=(11,), stride=(1,), scale=0.1093195378780365, zero_point=167, padding=(5,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0061]), zero_point=tensor([22]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.03407951816916466, zero_point=195, padding=(1,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0116]), zero_point=tensor([27]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.03575887903571129, zero_point=152, padding=(3,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0321]), zero_point=tensor([28]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.07003128528594971, zero_point=149, padding=(5,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0100]), zero_point=tensor([66]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.020489992573857307, zero_point=146, padding=(1,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0197]), zero_point=tensor([49]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.06123865395784378, zero_point=146, padding=(1,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0403]), zero_point=tensor([47]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(3,), stride=(1,), scale=0.08679024875164032, zero_point=146, padding=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0061]), zero_point=tensor([22]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.025142939761281013, zero_point=173, padding=(3,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0168]), zero_point=tensor([16]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.04650590196251869, zero_point=146, padding=(9,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0350]), zero_point=tensor([21]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.07499895244836807, zero_point=157, padding=(15,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0098]), zero_point=tensor([44]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.025900892913341522, zero_point=110, padding=(3,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0226]), zero_point=tensor([31]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.05694230645895004, zero_point=129, padding=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0335]), zero_point=tensor([35]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(7,), stride=(1,), scale=0.15990792214870453, zero_point=212, padding=(3,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlock1(\n",
       "        (convs1): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0061]), zero_point=tensor([22]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.023685740306973457, zero_point=198, padding=(5,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0081]), zero_point=tensor([25]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.02240218035876751, zero_point=159, padding=(15,), dilation=(3,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0201]), zero_point=tensor([33]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.0501457117497921, zero_point=167, padding=(25,), dilation=(5,))\n",
       "          )\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0073]), zero_point=tensor([69]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.014237510971724987, zero_point=135, padding=(5,))\n",
       "          )\n",
       "          (1): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0099]), zero_point=tensor([39]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.030032534152269363, zero_point=124, padding=(5,))\n",
       "          )\n",
       "          (2): QuantWrapper(\n",
       "            (quant): Quantize(scale=tensor([0.0193]), zero_point=tensor([48]), dtype=torch.quint8)\n",
       "            (dequant): DeQuantize()\n",
       "            (module): QuantizedConv1d(128, 128, kernel_size=(11,), stride=(1,), scale=0.050306711345911026, zero_point=165, padding=(5,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (reflection_pad): ReflectionPad1d((1, 0))\n",
       "    (subband_conv_post): QuantWrapper(\n",
       "      (quant): Quantize(scale=tensor([0.0212]), zero_point=tensor([21]), dtype=torch.quint8)\n",
       "      (dequant): DeQuantize()\n",
       "      (module): QuantizedConv1d(128, 72, kernel_size=(7,), stride=(1,), scale=0.015744032338261604, zero_point=110, padding=(3,))\n",
       "    )\n",
       "    (stft): TorchSTFT()\n",
       "  )\n",
       "  (enc_q): PosteriorEncoder(\n",
       "    (pre): Conv1d(80, 192, kernel_size=(1,), stride=(1,))\n",
       "    (enc): WN(\n",
       "      (in_layers): ModuleList(\n",
       "        (0-15): 16 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (res_skip_layers): ModuleList(\n",
       "        (0-14): 15 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "        (15): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "      (cond_layer): Conv1d(256, 6144, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (flow): ResidualCouplingTransformersBlock(\n",
       "    (flows): ModuleList(\n",
       "      (0): ResidualCouplingTransformersLayer2(\n",
       "        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): Flip()\n",
       "      (2): ResidualCouplingTransformersLayer2(\n",
       "        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): Flip()\n",
       "      (4): ResidualCouplingTransformersLayer2(\n",
       "        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (5): Flip()\n",
       "      (6): ResidualCouplingTransformersLayer2(\n",
       "        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))\n",
       "        (pre_transformer): Encoder(\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (attn_layers): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_1): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "          (ffn_layers): ModuleList(\n",
       "            (0): FFN(\n",
       "              (conv_1): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (conv_2): Conv1d(192, 192, kernel_size=(5,), stride=(1,))\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm_layers_2): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (enc): WN(\n",
       "          (in_layers): ModuleList(\n",
       "            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (res_skip_layers): ModuleList(\n",
       "            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
       "            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (cond_layer): Conv1d(256, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (7): Flip()\n",
       "    )\n",
       "  )\n",
       "  (dp): StochasticDurationPredictor(\n",
       "    (log_flow): Log()\n",
       "    (flows): ModuleList(\n",
       "      (0): ElementwiseAffine()\n",
       "      (1): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Flip()\n",
       "      (3): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (4): Flip()\n",
       "      (5): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (6): Flip()\n",
       "      (7): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (8): Flip()\n",
       "    )\n",
       "    (post_pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "    (post_proj): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (post_convs): DDSConv(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (convs_sep): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "      )\n",
       "      (convs_1x1): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (norms_1): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "      (norms_2): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (post_flows): ModuleList(\n",
       "      (0): ElementwiseAffine()\n",
       "      (1): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Flip()\n",
       "      (3): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (4): Flip()\n",
       "      (5): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (6): Flip()\n",
       "      (7): ConvFlow(\n",
       "        (pre): Conv1d(1, 256, kernel_size=(1,), stride=(1,))\n",
       "        (convs): DDSConv(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (convs_sep): ModuleList(\n",
       "            (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "            (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "            (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "          )\n",
       "          (convs_1x1): ModuleList(\n",
       "            (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norms_1): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "          (norms_2): ModuleList(\n",
       "            (0-2): 3 x LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (8): Flip()\n",
       "    )\n",
       "    (pre): Conv1d(192, 256, kernel_size=(1,), stride=(1,))\n",
       "    (proj): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (convs): DDSConv(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (convs_sep): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)\n",
       "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), groups=256)\n",
       "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,), groups=256)\n",
       "      )\n",
       "      (convs_1x1): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (norms_1): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "      (norms_2): ModuleList(\n",
       "        (0-2): 3 x LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (cond): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (emb_g): Embedding(5, 256)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from ptq import prepare_model_for_ptq_convs_only, convert_model_from_ptq\n",
    "from models import SynthesizerTrn\n",
    "import utils  #     hparams\n",
    "from torch.nn.utils.weight_norm import WeightNorm\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 1.   float-  \n",
    "net_g_q = SynthesizerTrn(\n",
    "    len(symbols),\n",
    "    80,\n",
    "    config['train']['segment_size'] // config['data']['hop_length'],\n",
    "    n_speakers=config['data']['n_speakers'],\n",
    "    mas_noise_scale_initial=0.01,\n",
    "    noise_scale_delta=2e-6,\n",
    "    **config['model'],\n",
    ").to(device)\n",
    "try:\n",
    "    net_g_q.dec.remove_weight_norm()\n",
    "except AttributeError:\n",
    "    pass \n",
    "\n",
    "#    -,    \n",
    "prepare_model_for_ptq_convs_only(\n",
    "    net_g_q,\n",
    "    module_roots=[\"dec\"],\n",
    "    backend=\"fbgemm\",\n",
    ")\n",
    "convert_model_from_ptq(net_g_q)\n",
    "\n",
    "#   \n",
    "checkpoint = torch.load(\"G_natasha_quantized_conv_only.pth\", map_location=device)\n",
    "missing, unexpected = net_g_q.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "print(\"MISSING:\", missing)\n",
    "print(\"UNEXPECTED:\", unexpected)\n",
    "\n",
    "#  weight_norm\n",
    "#fix_weight_norm_after_load(net_g_q)\n",
    "net_g_q.eval()\n",
    "\n",
    "# try:\n",
    "#     net_g_q.dec.remove_weight_norm()\n",
    "# except AttributeError:\n",
    "#     pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfa7ef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING: []\n",
      "UNEXPECTED: []\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"G_natasha_quantized_conv_only.pth\", map_location=device)\n",
    "\n",
    "missing, unexpected = net_g_q.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "print(\"MISSING:\", missing)\n",
    "print(\"UNEXPECTED:\", unexpected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "573dff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vcss_q(out, inputstr, i):  # single\n",
    "    stn_tst = get_text(inputstr, config)\n",
    "\n",
    "    speed = 1.0\n",
    "    output_dir = \"outputs\"\n",
    "    sid = torch.LongTensor([i]).to(device)\n",
    "    with torch.no_grad():\n",
    "        x_tst = stn_tst.to(device).unsqueeze(0)\n",
    "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "        audio = \\\n",
    "        net_g_q.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.667, noise_scale_w=0.8, length_scale=1 / speed)[0][\n",
    "            0, 0].data.cpu().numpy() * 32768.0  # vol scale\n",
    "        print(audio, np.max(audio))\n",
    "    write(rf'{output_dir}/{out}.wav', config['data']['sampling_rate'], audio.astype(np.int16))\n",
    "    print(rf'{out}.wav Generated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df7ff58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "[  51.630714   46.373856   37.759098 ... -188.15112   -90.40284\n",
      "   70.33666 ] 8499.258\n",
      "congrats_q_loaded.wav Generated!\n"
     ]
    }
   ],
   "source": [
    "vcss_q(\"congrats_q_loaded\", txt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91ef0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text.symbols import symbols\n",
    "# net_g_quant = models.SynthesizerTrn(\n",
    "#     len(symbols),\n",
    "#     80,\n",
    "#     config['train']['segment_size'] // config['data']['hop_length'],\n",
    "#     n_speakers=config['data']['n_speakers'],\n",
    "#     mas_noise_scale_initial=0.01,\n",
    "#     noise_scale_delta=2e-6,\n",
    "#     **config['model']).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "981ced46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.load_checkpoint(r\"/home/michael/Documents/ITMO/EDLM/phone-tts/pretrained/G_1000.pth\",\n",
    "#                     net_g_quant,\n",
    "#                     None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4705362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_g_quant.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8f182",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d71d7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load quantized encoder weights\n",
    "# enc_int8_path = \"/home/michael/Documents/ITMO/EDLM/phone-tts/G_natasha_quantized_dec.pth\"\n",
    "# sd = torch.load(str(enc_int8_path), map_location=\"cpu\")\n",
    "# net_g_quant.load_state_dict(sd['model'], strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bac56892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = '  + ++,   +.    -  +++  .'\n",
    "# out = 'congrats_quant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36f225fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_text(txt, config):\n",
    "#     text_norm = text.text_to_sequence_g2p(txt)\n",
    "#     if config['data']['add_blank']:\n",
    "#         text_norm = commons.intersperse(text_norm, 0)\n",
    "#     text_norm = torch.LongTensor(text_norm)\n",
    "#     print(text_norm)\n",
    "#     return text_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "886a26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_text(txt, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4524be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "congrats_prof.wav Generated!\n",
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "congrats_prof.wav Generated!\n",
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "congrats_prof.wav Generated!\n",
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "congrats_prof.wav Generated!\n",
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "congrats_prof.wav Generated!\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Helper to run vcss N times to accumulate profiler statistics\n",
    "def _run_vcss_n(n: int = 1):\n",
    "    for _ in range(n):\n",
    "        # Use a unique output name to avoid overwriting previous files\n",
    "        vcss(\"congrats_prof\", txt, 1)\n",
    "\n",
    "# Profile CPU ops during vcss; record_shapes helps attribute conv shapes\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"vcss_inference\"):\n",
    "        _run_vcss_n(n=5)  # increase to >=5 for more stable averages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db222996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 aten::_slow_conv2d_forward        35.94%     637.598ms        38.16%     677.038ms     115.045us          5885  \n",
      "                  aten::slow_conv_dilated2d        23.87%     423.510ms        28.73%     509.724ms      49.488us         10300  \n",
      "                             vcss_inference         7.60%     134.842ms       100.00%        1.774s        1.774s             1  \n",
      "                                aten::copy_         4.01%      71.083ms         4.01%      71.083ms       2.429us         29270  \n",
      "                aten::slow_conv_transpose2d         3.30%      58.526ms         3.57%      63.310ms       4.221ms            15  \n",
      "                               aten::select         2.36%      41.788ms         3.05%      54.025ms       0.802us         67335  \n",
      "               aten::_weight_norm_interface         2.24%      39.770ms         2.30%      40.786ms     107.333us           380  \n",
      "                                aten::fill_         1.94%      34.348ms         3.06%      54.297ms       2.417us         22465  \n",
      "                         aten::_convolution         1.89%      33.555ms        76.60%        1.359s       1.510ms           900  \n",
      "                                aten::slice         1.69%      29.980ms         2.41%      42.667ms       0.878us         48605  \n",
      "                                  aten::bmm         1.63%      29.000ms         1.71%      30.282ms     144.198us           210  \n",
      "                                  aten::add         1.59%      28.268ms         1.60%      28.388ms      34.832us           815  \n",
      "                           aten::as_strided         1.48%      26.200ms         1.48%      26.200ms       0.217us        120580  \n",
      "                               aten::narrow         1.22%      21.563ms         3.45%      61.276ms       1.308us         46865  \n",
      "                           aten::leaky_relu         1.20%      21.289ms         1.20%      21.289ms     109.173us           195  \n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.774s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show top operators by self CPU time\n",
    "print(prof.key_averages(group_by_stack_n=10).table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=15\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b870191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "[ -26.565493   -2.630526   25.153147 ... -173.40561   -69.34852\n",
      "   54.523773] 9401.616\n",
      "congrats_prof_q.wav Generated!\n",
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "[-129.61276  -126.079216 -212.50798  ...  -75.03236   -33.706177\n",
      "   19.48972 ] 9949.526\n",
      "congrats_prof_q.wav Generated!\n",
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "[-129.46712   -71.19141   -36.85733  ... -110.57238   -32.53287\n",
      "   41.603065] 9887.526\n",
      "congrats_prof_q.wav Generated!\n",
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "[   0.          0.          0.       ... -133.66338   -17.523945\n",
      "   17.424517] 7909.5093\n",
      "congrats_prof_q.wav Generated!\n",
      "tensor([ 1,  0, 32,  0, 14,  0,  3,  0, 52,  0, 23,  0, 33,  0, 47,  0, 51,  0,\n",
      "         3,  0, 47,  0, 27,  0, 22,  0, 40,  0, 22,  0, 46,  0, 30,  0, 45,  0,\n",
      "        41,  0, 55,  0, 14,  0, 39,  0, 39,  0, 57,  0, 32,  0,  3,  0, 37,  0,\n",
      "        41,  0, 21,  0, 23,  0, 36,  0, 32,  0, 53,  0,  8,  0,  3,  0, 59,  0,\n",
      "        14,  0, 26,  0, 45,  0, 54,  0, 60,  0, 22,  0, 39,  0, 39,  0, 41,  0,\n",
      "        32,  0,  3,  0, 31,  0, 59,  0,  3,  0, 19,  0, 22,  0, 33,  0, 43,  0,\n",
      "        42,  0, 30,  0, 39,  0, 51,  0, 14,  0, 10,  0,  3,  0, 32,  0, 23,  0,\n",
      "        47,  0, 36,  0, 30,  0,  3,  0, 38,  0, 22,  0, 40,  0, 15,  0,  3,  0,\n",
      "        41,  0, 59,  0, 55,  0, 54,  0, 19,  0, 30,  0, 36,  0, 30,  0,  3,  0,\n",
      "         9,  0,  3,  0, 41,  0, 51,  0, 43,  0, 45,  0, 14,  0, 56,  0, 30,  0,\n",
      "        55,  0, 49,  0, 30,  0, 32,  0,  3,  0, 32,  0, 22,  0, 26,  0, 41,  0,\n",
      "         3,  0, 20,  0, 41,  0, 47,  0, 51,  0, 42,  0, 30,  0, 39,  0,  3,  0,\n",
      "        53,  0, 55,  0, 14,  0, 60,  0, 23,  0, 40,  0, 30,  0, 32,  0, 14,  0,\n",
      "        10,  0,  2])\n",
      "[-78.93039    19.227987    5.4842296 ...   3.6954067   5.086013\n",
      "  -0.6171013] 9244.557\n",
      "congrats_prof_q.wav Generated!\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Helper to run vcss N times to accumulate profiler statistics\n",
    "def _run_vcss_n(n: int = 1):\n",
    "    for _ in range(n):\n",
    "        # Use a unique output name to avoid overwriting previous files\n",
    "        vcss_q(\"congrats_prof_q\", txt, 1)\n",
    "\n",
    "# Profile CPU ops during vcss; record_shapes helps attribute conv shapes\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"vcss_inference\"):\n",
    "        _run_vcss_n(n=5)  # increase to >=5 for more stable averages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bd58318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                               Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  quantized::conv1d        38.61%     680.790ms        39.48%     696.101ms       3.664ms           190  \n",
      "         aten::_slow_conv2d_forward        11.90%     209.785ms        13.34%     235.144ms      40.859us          5755  \n",
      "          aten::slow_conv_dilated2d        10.80%     190.487ms        13.45%     237.082ms      23.153us         10240  \n",
      "        quantized::conv_transpose1d         9.14%     161.214ms         9.17%     161.752ms      16.175ms            10  \n",
      "                     vcss_inference         4.27%      75.282ms       100.00%        1.763s        1.763s             1  \n",
      "                        aten::copy_         2.48%      43.667ms         2.48%      43.667ms       2.478us         17625  \n",
      "                        aten::clone         2.23%      39.245ms         2.91%      51.346ms      63.784us           805  \n",
      "                 aten::_convolution         1.62%      28.520ms        32.22%     568.117ms     811.596us           700  \n",
      "                        aten::slice         1.55%      27.330ms         2.05%      36.111ms       0.743us         48605  \n",
      "                       aten::select         1.53%      26.985ms         1.88%      33.089ms       0.750us         44145  \n",
      "                          aten::add         1.37%      24.210ms         1.38%      24.336ms      29.860us           815  \n",
      "                          aten::bmm         1.32%      23.355ms         1.38%      24.318ms     115.798us           210  \n",
      "                        aten::fill_         1.13%      19.963ms         1.45%      25.621ms       2.343us         10935  \n",
      "                       aten::narrow         1.13%      19.861ms         3.04%      53.684ms       1.145us         46865  \n",
      "                   aten::leaky_relu         0.94%      16.521ms         0.94%      16.521ms      84.722us           195  \n",
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.763s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show top operators by self CPU time\n",
    "print(prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by=\"self_cpu_time_total\", row_limit=15\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa274a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
